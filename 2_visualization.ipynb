{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 2: Data Visualization and Exploratory Data Analysis\n",
                "\n",
                "## ðŸŽ¯ Objective\n",
                "\n",
                "This notebook focuses on understanding the UNSW-NB15 dataset through comprehensive visualizations. We will:\n",
                "\n",
                "1. Load the preprocessed data\n",
                "2. Examine dataset statistics and distributions\n",
                "3. Visualize class imbalance\n",
                "4. Analyze feature distributions (normal vs attack)\n",
                "5. Create correlation heatmaps\n",
                "6. Visualize sequence patterns\n",
                "7. Generate insights for model development\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Why Visualization Matters in Cybersecurity\n",
                "\n",
                "**In intrusion detection:**\n",
                "- Helps identify distinguishing features between normal and attack traffic\n",
                "- Reveals data quality issues early\n",
                "- Guides feature engineering decisions\n",
                "- Provides insights into attack patterns\n",
                "- Validates preprocessing steps\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Required Libraries\n",
                "\n",
                "We'll use:\n",
                "- **seaborn**: Statistical data visualization (primary plotting library)\n",
                "- **matplotlib**: Base plotting functionality\n",
                "- **pandas & numpy**: Data manipulation\n",
                "- **pickle**: Load preprocessed objects"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pickle\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style for better-looking plots\n",
                "sns.set_style('whitegrid')\n",
                "sns.set_palette('husl')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 10\n",
                "\n",
                "print(\"âœ… Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Preprocessed Data\n",
                "\n",
                "We'll load the data that was saved in Notebook 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load sequences\n",
                "print(\"ðŸ“‚ Loading preprocessed data...\")\n",
                "X_train = np.load('preprocessed_data/X_train.npy')\n",
                "X_train_normal = np.load('preprocessed_data/X_train_normal.npy')\n",
                "X_val = np.load('preprocessed_data/X_val.npy')\n",
                "X_test = np.load('preprocessed_data/X_test.npy')\n",
                "y_train = np.load('preprocessed_data/y_train.npy')\n",
                "y_val = np.load('preprocessed_data/y_val.npy')\n",
                "y_test = np.load('preprocessed_data/y_test.npy')\n",
                "\n",
                "# Load feature names\n",
                "with open('preprocessed_data/feature_names.pkl', 'rb') as f:\n",
                "    feature_names = pickle.load(f)\n",
                "\n",
                "print(f\"\\nâœ… Data loaded successfully!\")\n",
                "print(f\"   - Training sequences: {X_train.shape}\")\n",
                "print(f\"   - Validation sequences: {X_val.shape}\")\n",
                "print(f\"   - Test sequences: {X_test.shape}\")\n",
                "print(f\"   - Number of features: {len(feature_names)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Dataset Overview Statistics\n",
                "\n",
                "Let's examine the basic statistics of our preprocessed dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine all data for overall statistics\n",
                "X_all = np.concatenate([X_train, X_val, X_test], axis=0)\n",
                "y_all = np.concatenate([y_train, y_val, y_test], axis=0)\n",
                "\n",
                "print(\"ðŸ“Š Dataset Statistics:\")\n",
                "print(f\"   - Total sequences: {len(X_all):,}\")\n",
                "print(f\"   - Sequence length: {X_all.shape[1]} flows\")\n",
                "print(f\"   - Features per flow: {X_all.shape[2]}\")\n",
                "print(f\"\\n   - Normal sequences: {np.sum(y_all == 0):,} ({np.sum(y_all == 0) / len(y_all) * 100:.2f}%)\")\n",
                "print(f\"   - Attack sequences: {np.sum(y_all == 1):,} ({np.sum(y_all == 1) / len(y_all) * 100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Visualize Class Distribution\n",
                "\n",
                "Understanding class balance is crucial for:\n",
                "- Identifying potential bias in the model\n",
                "- Deciding on sampling strategies\n",
                "- Choosing appropriate evaluation metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create figure with subplots for train, val, and test distributions\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "datasets = [\n",
                "    ('Training Set', y_train),\n",
                "    ('Validation Set', y_val),\n",
                "    ('Test Set', y_test)\n",
                "]\n",
                "\n",
                "for idx, (title, labels) in enumerate(datasets):\n",
                "    # Count normal and attack sequences\n",
                "    counts = pd.Series(labels).value_counts().sort_index()\n",
                "    \n",
                "    # Create bar plot\n",
                "    sns.barplot(x=['Normal', 'Attack'], y=counts.values, ax=axes[idx], palette='Set2')\n",
                "    axes[idx].set_title(f'{title}\\n({len(labels):,} sequences)', fontsize=12, fontweight='bold')\n",
                "    axes[idx].set_ylabel('Number of Sequences', fontsize=10)\n",
                "    axes[idx].set_xlabel('Class', fontsize=10)\n",
                "    \n",
                "    # Add count labels on bars\n",
                "    for i, v in enumerate(counts.values):\n",
                "        axes[idx].text(i, v + len(labels) * 0.02, f'{v:,}\\n({v/len(labels)*100:.1f}%)', \n",
                "                      ha='center', va='bottom', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Class Distribution Across Datasets', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Class distribution visualized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Feature Distribution Analysis\n",
                "\n",
                "We'll analyze how feature distributions differ between normal and attack traffic. This helps identify discriminative features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Flatten sequences to get individual flows for feature analysis\n",
                "# Take the first flow from each sequence for simplicity\n",
                "X_train_flat = X_train[:, 0, :]  # Shape: (num_sequences, num_features)\n",
                "\n",
                "# Create DataFrame for easier plotting\n",
                "df_viz = pd.DataFrame(X_train_flat, columns=feature_names)\n",
                "df_viz['label'] = y_train\n",
                "df_viz['class'] = df_viz['label'].map({0: 'Normal', 1: 'Attack'})\n",
                "\n",
                "print(f\"âœ… Created visualization DataFrame with shape: {df_viz.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select a few important features to visualize\n",
                "# You can adjust these based on your dataset's actual feature names\n",
                "features_to_plot = feature_names[:6]  # First 6 features as example\n",
                "\n",
                "# Create distribution plots\n",
                "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, feature in enumerate(features_to_plot):\n",
                "    # Create histogram with KDE for normal and attack traffic\n",
                "    sns.histplot(data=df_viz, x=feature, hue='class', kde=True, \n",
                "                 ax=axes[idx], palette='Set1', alpha=0.6, bins=30)\n",
                "    axes[idx].set_title(f'Distribution of {feature}', fontsize=11, fontweight='bold')\n",
                "    axes[idx].set_xlabel(feature, fontsize=9)\n",
                "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
                "    axes[idx].legend(title='Traffic Type', fontsize=8)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Feature Distributions: Normal vs Attack Traffic', \n",
                "             fontsize=14, fontweight='bold', y=1.005)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Feature distributions plotted!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Box Plots for Feature Comparison\n",
                "\n",
                "Box plots help visualize:\n",
                "- Median values\n",
                "- Quartile ranges\n",
                "- Outliers\n",
                "- Differences between normal and attack traffic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create box plots for selected features\n",
                "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, feature in enumerate(features_to_plot):\n",
                "    sns.boxplot(data=df_viz, x='class', y=feature, ax=axes[idx], palette='Set2')\n",
                "    axes[idx].set_title(f'Box Plot: {feature}', fontsize=11, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Traffic Type', fontsize=9)\n",
                "    axes[idx].set_ylabel(feature, fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Feature Comparison via Box Plots: Normal vs Attack', \n",
                "             fontsize=14, fontweight='bold', y=1.005)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Box plots created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Correlation Heatmap\n",
                "\n",
                "Correlation analysis helps:\n",
                "- Identify redundant features\n",
                "- Understand feature relationships\n",
                "- Detect multicollinearity\n",
                "- Guide feature selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate correlation matrix\n",
                "# Use a subset of features for better visibility\n",
                "num_features_to_show = min(20, len(feature_names))\n",
                "correlation_features = feature_names[:num_features_to_show]\n",
                "\n",
                "correlation_matrix = df_viz[correlation_features].corr()\n",
                "\n",
                "# Create heatmap\n",
                "plt.figure(figsize=(14, 12))\n",
                "sns.heatmap(correlation_matrix, \n",
                "            annot=True, \n",
                "            fmt='.2f', \n",
                "            cmap='coolwarm', \n",
                "            center=0,\n",
                "            square=True,\n",
                "            linewidths=0.5,\n",
                "            cbar_kws={'shrink': 0.8})\n",
                "\n",
                "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Correlation heatmap created!\")\n",
                "print(f\"\\nðŸ” Highly correlated feature pairs (|r| > 0.8):\")\n",
                "\n",
                "# Find highly correlated features\n",
                "high_corr = []\n",
                "for i in range(len(correlation_matrix.columns)):\n",
                "    for j in range(i+1, len(correlation_matrix.columns)):\n",
                "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
                "            high_corr.append((\n",
                "                correlation_matrix.columns[i], \n",
                "                correlation_matrix.columns[j], \n",
                "                correlation_matrix.iloc[i, j]\n",
                "            ))\n",
                "\n",
                "if high_corr:\n",
                "    for feat1, feat2, corr_val in high_corr[:10]:  # Show top 10\n",
                "        print(f\"   {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
                "else:\n",
                "    print(\"   No highly correlated pairs found (threshold: |r| > 0.8)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Visualize Sequence Patterns\n",
                "\n",
                "Since our model works on sequences, let's visualize how features evolve over time in sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select random normal and attack sequences\n",
                "normal_idx = np.where(y_train == 0)[0][0]\n",
                "attack_idx = np.where(y_train == 1)[0][0]\n",
                "\n",
                "normal_seq = X_train[normal_idx]\n",
                "attack_seq = X_train[attack_idx]\n",
                "\n",
                "# Plot first 4 features over the sequence\n",
                "features_to_show = min(4, len(feature_names))\n",
                "\n",
                "fig, axes = plt.subplots(2, features_to_show, figsize=(16, 8))\n",
                "\n",
                "for feat_idx in range(features_to_show):\n",
                "    # Normal sequence\n",
                "    axes[0, feat_idx].plot(normal_seq[:, feat_idx], marker='o', color='green', linewidth=2)\n",
                "    axes[0, feat_idx].set_title(f'Normal: {feature_names[feat_idx]}', fontsize=10)\n",
                "    axes[0, feat_idx].set_xlabel('Flow Position', fontsize=8)\n",
                "    axes[0, feat_idx].set_ylabel('Value', fontsize=8)\n",
                "    axes[0, feat_idx].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Attack sequence\n",
                "    axes[1, feat_idx].plot(attack_seq[:, feat_idx], marker='o', color='red', linewidth=2)\n",
                "    axes[1, feat_idx].set_title(f'Attack: {feature_names[feat_idx]}', fontsize=10)\n",
                "    axes[1, feat_idx].set_xlabel('Flow Position', fontsize=8)\n",
                "    axes[1, feat_idx].set_ylabel('Value', fontsize=8)\n",
                "    axes[1, feat_idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Temporal Patterns: Normal vs Attack Sequences', \n",
                "             fontsize=14, fontweight='bold', y=1.002)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Sequence patterns visualized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Feature Statistics Summary\n",
                "\n",
                "Let's create a summary table comparing normal vs attack traffic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate statistics for normal and attack traffic\n",
                "summary_data = []\n",
                "\n",
                "for feature in features_to_plot:\n",
                "    normal_data = df_viz[df_viz['class'] == 'Normal'][feature]\n",
                "    attack_data = df_viz[df_viz['class'] == 'Attack'][feature]\n",
                "    \n",
                "    summary_data.append({\n",
                "        'Feature': feature,\n",
                "        'Normal Mean': normal_data.mean(),\n",
                "        'Attack Mean': attack_data.mean(),\n",
                "        'Normal Std': normal_data.std(),\n",
                "        'Attack Std': attack_data.std(),\n",
                "        'Difference': abs(normal_data.mean() - attack_data.mean())\n",
                "    })\n",
                "\n",
                "summary_df = pd.DataFrame(summary_data)\n",
                "summary_df = summary_df.sort_values('Difference', ascending=False)\n",
                "\n",
                "print(\"ðŸ“Š Feature Statistics Summary (sorted by discrimination power):\")\n",
                "print(summary_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Violin Plots for Dense Distribution View\n",
                "\n",
                "Violin plots combine box plots with kernel density estimation for richer insights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create violin plots for top discriminative features\n",
                "top_features = summary_df.head(4)['Feature'].tolist()\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, feature in enumerate(top_features):\n",
                "    sns.violinplot(data=df_viz, x='class', y=feature, ax=axes[idx], palette='muted')\n",
                "    axes[idx].set_title(f'Violin Plot: {feature}', fontsize=11, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Traffic Type', fontsize=9)\n",
                "    axes[idx].set_ylabel(feature, fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Violin Plots: Most Discriminative Features', \n",
                "             fontsize=14, fontweight='bold', y=1.002)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ… Violin plots created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Summary of Insights\n",
                "\n",
                "From our exploratory data analysis:\n",
                "\n",
                "### Key Findings:\n",
                "\n",
                "1. **Class Distribution**:\n",
                "   - The dataset shows class imbalance between normal and attack traffic\n",
                "   - This is realistic for intrusion detection scenarios\n",
                "   - Our autoencoder approach is well-suited for this imbalance\n",
                "\n",
                "2. **Feature Discriminability**:\n",
                "   - Some features show clear differences between normal and attack patterns\n",
                "   - These will be valuable for the model's learning process\n",
                "\n",
                "3. **Temporal Patterns**:\n",
                "   - Sequences show varying patterns over time\n",
                "   - LSTM will be crucial for capturing these temporal dependencies\n",
                "\n",
                "4. **Feature Correlations**:\n",
                "   - Some features are highly correlated (potential redundancy)\n",
                "   - The CNN layers will help extract relevant combined features\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Next Steps\n",
                "\n",
                "Proceed to **Notebook 3: Model Training** to:\n",
                "- Build the CNN+LSTM Autoencoder architecture\n",
                "- Train the model on normal traffic\n",
                "- Detect intrusions using reconstruction error\n",
                "- Evaluate performance with comprehensive metrics\n",
                "\n",
                "---\n",
                "\n",
                "**The insights from this visualization will guide our model design and hyperparameter choices!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
