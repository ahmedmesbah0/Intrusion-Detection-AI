{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Model Training and Evaluation\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "This notebook implements and trains our hybrid **1D CNN + LSTM Autoencoder** for intrusion detection. We will:\n",
    "\n",
    "1. Load preprocessed sequential data\n",
    "2. Build the CNN+LSTM Autoencoder architecture\n",
    "3. Train the model on normal traffic only\n",
    "4. Evaluate using reconstruction error\n",
    "5. Detect intrusions (anomalies)\n",
    "6. Generate comprehensive performance metrics\n",
    "7. Visualize results and model performance\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Background: Autoencoder-Based Anomaly Detection\n",
    "\n",
    "### What is an Autoencoder?\n",
    "An autoencoder is an unsupervised neural network that:\n",
    "- **Encodes** input data into a compressed latent representation\n",
    "- **Decodes** the latent representation back to original format\n",
    "- Learns to **reconstruct** its input\n",
    "\n",
    "### Why for Intrusion Detection?\n",
    "1. Train only on **normal traffic** (no need for labeled attacks during training)\n",
    "2. Model learns patterns of normal behavior\n",
    "3. Attack traffic will have **high reconstruction error** (anomaly)\n",
    "4. Set threshold: high error = intrusion detected!\n",
    "\n",
    "### Our Hybrid Architecture:\n",
    "- **1D CNN layers**: Extract spatial features from each flow's features\n",
    "- **LSTM Encoder**: Capture temporal dependencies across sequence\n",
    "- **LSTM Decoder**: Reconstruct the original sequence\n",
    "- **Output**: Reconstructed sequence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We'll use TensorFlow/Keras for building our deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/mesbah7/Github/Repos/Intrusion-Detection-AI/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, RepeatVector, TimeDistributed, Dropout\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed Data\n",
    "\n",
    "We'll load the sequences created in Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed sequences\n",
    "print(\"üìÇ Loading preprocessed data...\")\n",
    "X_train = np.load('preprocessed_data/X_train.npy')\n",
    "X_train_normal = np.load('preprocessed_data/X_train_normal.npy')\n",
    "X_val = np.load('preprocessed_data/X_val.npy')\n",
    "X_test = np.load('preprocessed_data/X_test.npy')\n",
    "y_train = np.load('preprocessed_data/y_train.npy')\n",
    "y_val = np.load('preprocessed_data/y_val.npy')\n",
    "y_test = np.load('preprocessed_data/y_test.npy')\n",
    "\n",
    "# Load feature names\n",
    "with open('preprocessed_data/feature_names.pkl', 'rb') as f:\n",
    "    feature_names = pickle.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"\\nData Shapes:\")\n",
    "print(f\"   - X_train: {X_train.shape}\")\n",
    "print(f\"   - X_train_normal (for autoencoder): {X_train_normal.shape}\")\n",
    "print(f\"   - X_val: {X_val.shape}\")\n",
    "print(f\"   - X_test: {X_test.shape}\")\n",
    "print(f\"\\nSequence Info:\")\n",
    "print(f\"   - Sequence length: {X_train.shape[1]} flows\")\n",
    "print(f\"   - Features per flow: {X_train.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the CNN+LSTM Autoencoder Architecture\n",
    "\n",
    "Our model consists of:\n",
    "\n",
    "### Encoder:\n",
    "1. **1D CNN layers**: Extract features from each time step\n",
    "2. **LSTM layers**: Encode temporal dependencies\n",
    "\n",
    "### Decoder:\n",
    "1. **RepeatVector**: Repeat latent representation for each time step\n",
    "2. **LSTM layers**: Decode temporal information\n",
    "3. **TimeDistributed Dense**: Reconstruct features for each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_autoencoder(sequence_length, n_features, latent_dim=32):\n",
    "    \"\"\"\n",
    "    Build a hybrid 1D CNN + LSTM Autoencoder for sequence anomaly detection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequence_length : int\n",
    "        Length of input sequences (number of time steps)\n",
    "    n_features : int\n",
    "        Number of features at each time step\n",
    "    latent_dim : int\n",
    "        Dimension of the latent (compressed) representation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled autoencoder model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(sequence_length, n_features))\n",
    "    \n",
    "    # =====================\n",
    "    # ENCODER\n",
    "    # =====================\n",
    "    \n",
    "    # 1D CNN layers for spatial feature extraction\n",
    "    # These extract patterns within each time step's features\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # LSTM Encoder - captures temporal dependencies\n",
    "    # return_sequences=False means only return the final hidden state\n",
    "    encoded = LSTM(latent_dim, activation='relu', name='encoder_lstm')(x)\n",
    "    \n",
    "    # =====================\n",
    "    # DECODER\n",
    "    # =====================\n",
    "    \n",
    "    # Repeat the latent representation for each time step\n",
    "    x = RepeatVector(sequence_length)(encoded)\n",
    "    \n",
    "    # LSTM Decoder - reconstructs temporal patterns\n",
    "    x = LSTM(latent_dim, activation='relu', return_sequences=True)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = LSTM(32, activation='relu', return_sequences=True)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # TimeDistributed Dense layer to reconstruct features at each time step\n",
    "    decoded = TimeDistributed(Dense(n_features))(x)\n",
    "    \n",
    "    # =====================\n",
    "    # CREATE MODEL\n",
    "    # =====================\n",
    "    \n",
    "    autoencoder = Model(inputs, decoded, name='CNN_LSTM_Autoencoder')\n",
    "    \n",
    "    # Compile with MSE loss (reconstruction error)\n",
    "    autoencoder.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Get dimensions from data\n",
    "sequence_length = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# Build the model\n",
    "print(\"üèóÔ∏è Building CNN+LSTM Autoencoder...\")\n",
    "autoencoder = build_cnn_lstm_autoencoder(\n",
    "    sequence_length=sequence_length,\n",
    "    n_features=n_features,\n",
    "    latent_dim=32\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model built successfully!\")\n",
    "print(\"\\nüìä Model Summary:\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Model Architecture\n",
    "\n",
    "Let's visualize our model structure to better understand the data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model architecture\n",
    "keras.utils.plot_model(\n",
    "    autoencoder,\n",
    "    to_file='saved_models/model_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model architecture saved to 'saved_models/model_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Training Callbacks\n",
    "\n",
    "Callbacks help us:\n",
    "- Save the best model\n",
    "- Stop training early if no improvement\n",
    "- Reduce learning rate when stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving models\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'saved_models/best_autoencoder.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callback_list = [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "print(\"‚úÖ Training callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Autoencoder on Normal Traffic\n",
    "\n",
    "**Key Point**: We train ONLY on normal traffic so the model learns normal behavior patterns. Attack traffic will have high reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract normal sequences from validation set for validation during training\n",
    "X_val_normal = X_val[y_val == 0]\n",
    "\n",
    "print(f\"üéØ Training on NORMAL traffic only:\")\n",
    "print(f\"   - Training samples: {X_train_normal.shape[0]:,}\")\n",
    "print(f\"   - Validation samples: {X_val_normal.shape[0]:,}\")\n",
    "print(f\"\\nüöÄ Starting training...\\n\")\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(\n",
    "    X_train_normal, X_train_normal,  # Input = Output for autoencoder\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_normal, X_val_normal),\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training History\n",
    "\n",
    "Let's examine how the model's loss evolved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss During Training', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=10)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=10)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_title('Mean Absolute Error During Training', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=10)\n",
    "axes[1].set_ylabel('MAE', fontsize=10)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Calculate Reconstruction Errors\n",
    "\n",
    "Now we'll:\n",
    "1. Use the trained model to reconstruct sequences\n",
    "2. Calculate reconstruction error (MSE) for each sequence\n",
    "3. Use these errors to distinguish normal from attack traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(model, X):\n",
    "    \"\"\"\n",
    "    Calculate reconstruction error for each sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained autoencoder\n",
    "    X : ndarray\n",
    "        Input sequences\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    errors : ndarray\n",
    "        Reconstruction error (MSE) for each sequence\n",
    "    \"\"\"\n",
    "    # Get reconstructions\n",
    "    reconstructions = model.predict(X, verbose=0)\n",
    "    \n",
    "    # Calculate MSE for each sequence\n",
    "    errors = np.mean(np.square(X - reconstructions), axis=(1, 2))\n",
    "    \n",
    "    return errors\n",
    "\n",
    "print(\"üîç Calculating reconstruction errors...\")\n",
    "\n",
    "# Calculate errors for all datasets\n",
    "train_errors = calculate_reconstruction_error(autoencoder, X_train)\n",
    "val_errors = calculate_reconstruction_error(autoencoder, X_val)\n",
    "test_errors = calculate_reconstruction_error(autoencoder, X_test)\n",
    "\n",
    "print(\"‚úÖ Reconstruction errors calculated!\")\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"   - Train errors: min={train_errors.min():.6f}, max={train_errors.max():.6f}, mean={train_errors.mean():.6f}\")\n",
    "print(f\"   - Val errors: min={val_errors.min():.6f}, max={val_errors.max():.6f}, mean={val_errors.mean():.6f}\")\n",
    "print(f\"   - Test errors: min={test_errors.min():.6f}, max={test_errors.max():.6f}, mean={test_errors.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Set Anomaly Detection Threshold\n",
    "\n",
    "We need to set a threshold:\n",
    "- Sequences with error **below threshold** ‚Üí Normal\n",
    "- Sequences with error **above threshold** ‚Üí Attack (Anomaly)\n",
    "\n",
    "We'll use the 95th percentile of normal traffic errors in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get errors for normal traffic in validation set\n",
    "val_normal_errors = val_errors[y_val == 0]\n",
    "val_attack_errors = val_errors[y_val == 1]\n",
    "\n",
    "# Set threshold at 95th percentile of normal errors\n",
    "threshold = np.percentile(val_normal_errors, 95)\n",
    "\n",
    "print(f\"üéØ Anomaly Detection Threshold: {threshold:.6f}\")\n",
    "print(f\"\\nValidation Set Analysis:\")\n",
    "print(f\"   - Normal traffic errors: mean={val_normal_errors.mean():.6f}, std={val_normal_errors.std():.6f}\")\n",
    "print(f\"   - Attack traffic errors: mean={val_attack_errors.mean():.6f}, std={val_attack_errors.std():.6f}\")\n",
    "print(f\"\\n   - Normal sequences above threshold: {np.sum(val_normal_errors > threshold)} / {len(val_normal_errors)} ({np.sum(val_normal_errors > threshold) / len(val_normal_errors) * 100:.1f}%)\")\n",
    "print(f\"   - Attack sequences above threshold: {np.sum(val_attack_errors > threshold)} / {len(val_attack_errors)} ({np.sum(val_attack_errors > threshold) / len(val_attack_errors) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Reconstruction Error Distributions\n",
    "\n",
    "Let's visualize how well reconstruction error separates normal from attack traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Test set errors\n",
    "test_normal_errors = test_errors[y_test == 0]\n",
    "test_attack_errors = test_errors[y_test == 1]\n",
    "\n",
    "# Histogram with KDE\n",
    "axes[0].hist(test_normal_errors, bins=50, alpha=0.6, label='Normal', color='green', density=True)\n",
    "axes[0].hist(test_attack_errors, bins=50, alpha=0.6, label='Attack', color='red', density=True)\n",
    "axes[0].axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')\n",
    "axes[0].set_xlabel('Reconstruction Error (MSE)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Reconstruction Error Distribution (Test Set)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "error_df = pd.DataFrame({\n",
    "    'Error': np.concatenate([test_normal_errors, test_attack_errors]),\n",
    "    'Class': ['Normal'] * len(test_normal_errors) + ['Attack'] * len(test_attack_errors)\n",
    "})\n",
    "\n",
    "sns.boxplot(data=error_df, x='Class', y='Error', ax=axes[1], palette='Set2')\n",
    "axes[1].axhline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')\n",
    "axes[1].set_title('Reconstruction Error Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Reconstruction Error (MSE)', fontsize=11)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/reconstruction_error_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Error distributions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Make Predictions on Test Set\n",
    "\n",
    "Apply the threshold to classify sequences as normal or attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict based on threshold\n",
    "y_pred_test = (test_errors > threshold).astype(int)\n",
    "\n",
    "print(\"üéØ Predictions on Test Set:\")\n",
    "print(f\"   - Total sequences: {len(y_test):,}\")\n",
    "print(f\"   - Predicted Normal: {np.sum(y_pred_test == 0):,}\")\n",
    "print(f\"   - Predicted Attack: {np.sum(y_pred_test == 1):,}\")\n",
    "print(f\"\\n   - Actual Normal: {np.sum(y_test == 0):,}\")\n",
    "print(f\"   - Actual Attack: {np.sum(y_test == 1):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate Model Performance\n",
    "\n",
    "Calculate key metrics:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: Of predicted attacks, how many are actual attacks?\n",
    "- **Recall**: Of actual attacks, how many did we detect?\n",
    "- **F1-Score**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"üìä Model Performance Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f} ({precision * 100:.2f}%)\")\n",
    "print(f\"   Recall:    {recall:.4f} ({recall * 100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Normal', 'Attack']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Confusion Matrix\n",
    "\n",
    "Visualize true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Attack'],\n",
    "            yticklabels=['Normal', 'Attack'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Intrusion Detection', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add percentage annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm.sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix visualized!\")\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"   - True Negatives (TN): {cm[0, 0]:,}\")\n",
    "print(f\"   - False Positives (FP): {cm[0, 1]:,}\")\n",
    "print(f\"   - False Negatives (FN): {cm[1, 0]:,}\")\n",
    "print(f\"   - True Positives (TP): {cm[1, 1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: ROC Curve and AUC\n",
    "\n",
    "ROC (Receiver Operating Characteristic) curve shows the trade-off between true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, test_errors)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Intrusion Detection System', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if roc_auc > 0.9:\n",
    "    print(\"   üéâ Excellent discrimination between normal and attack traffic!\")\n",
    "elif roc_auc > 0.8:\n",
    "    print(\"   üëç Good discrimination capability.\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(\"   ‚ö†Ô∏è Fair discrimination, room for improvement.\")\n",
    "else:\n",
    "    print(\"   ‚ùå Poor discrimination, model needs improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Precision-Recall Curve\n",
    "\n",
    "Particularly useful for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision-Recall curve\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, test_errors)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('saved_models/precision_recall_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Precision-Recall AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Visualize Sample Reconstructions\n",
    "\n",
    "Let's see how well the autoencoder reconstructs normal vs attack sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample sequences\n",
    "normal_sample_idx = np.where(y_test == 0)[0][0]\n",
    "attack_sample_idx = np.where(y_test == 1)[0][0]\n",
    "\n",
    "# Get original and reconstructed sequences\n",
    "normal_original = X_test[normal_sample_idx]\n",
    "attack_original = X_test[attack_sample_idx]\n",
    "\n",
    "normal_reconstructed = autoencoder.predict(X_test[normal_sample_idx:normal_sample_idx+1], verbose=0)[0]\n",
    "attack_reconstructed = autoencoder.predict(X_test[attack_sample_idx:attack_sample_idx+1], verbose=0)[0]\n",
    "\n",
    "# Plot first 3 features for visualization\n",
    "features_to_plot = min(3, n_features)\n",
    "\n",
    "fig, axes = plt.subplots(2, features_to_plot, figsize=(15, 8))\n",
    "\n",
    "for feat_idx in range(features_to_plot):\n",
    "    # Normal sequence\n",
    "    axes[0, feat_idx].plot(normal_original[:, feat_idx], 'o-', label='Original', color='green', linewidth=2, markersize=6)\n",
    "    axes[0, feat_idx].plot(normal_reconstructed[:, feat_idx], 's--', label='Reconstructed', color='lightgreen', linewidth=2, markersize=6)\n",
    "    axes[0, feat_idx].set_title(f'Normal - Feature {feat_idx}\\nError: {test_errors[normal_sample_idx]:.6f}', fontsize=10)\n",
    "    axes[0, feat_idx].set_xlabel('Flow Position')\n",
    "    axes[0, feat_idx].set_ylabel('Value')\n",
    "    axes[0, feat_idx].legend()\n",
    "    axes[0, feat_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attack sequence\n",
    "    axes[1, feat_idx].plot(attack_original[:, feat_idx], 'o-', label='Original', color='red', linewidth=2, markersize=6)\n",
    "    axes[1, feat_idx].plot(attack_reconstructed[:, feat_idx], 's--', label='Reconstructed', color='pink', linewidth=2, markersize=6)\n",
    "    axes[1, feat_idx].set_title(f'Attack - Feature {feat_idx}\\nError: {test_errors[attack_sample_idx]:.6f}', fontsize=10)\n",
    "    axes[1, feat_idx].set_xlabel('Flow Position')\n",
    "    axes[1, feat_idx].set_ylabel('Value')\n",
    "    axes[1, feat_idx].legend()\n",
    "    axes[1, feat_idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Reconstructions: Normal vs Attack', fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.savefig('saved_models/sample_reconstructions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample reconstructions visualized!\")\n",
    "print(f\"\\nObservations:\")\n",
    "print(f\"   - Normal sequence reconstruction error: {test_errors[normal_sample_idx]:.6f} (below threshold)\")\n",
    "print(f\"   - Attack sequence reconstruction error: {test_errors[attack_sample_idx]:.6f} (above threshold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Save Final Results Summary\n",
    "\n",
    "Let's create a comprehensive summary of our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "results_summary = {\n",
    "    'Model Architecture': 'CNN + LSTM Autoencoder',\n",
    "    'Sequence Length': sequence_length,\n",
    "    'Number of Features': n_features,\n",
    "    'Training Samples (Normal)': X_train_normal.shape[0],\n",
    "    'Test Samples': len(y_test),\n",
    "    'Detection Threshold': threshold,\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'ROC AUC': roc_auc,\n",
    "    'PR AUC': pr_auc,\n",
    "    'True Negatives': int(cm[0, 0]),\n",
    "    'False Positives': int(cm[0, 1]),\n",
    "    'False Negatives': int(cm[1, 0]),\n",
    "    'True Positives': int(cm[1, 1])\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('saved_models/results_summary.txt', 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"INTRUSION DETECTION SYSTEM - RESULTS SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    for key, value in results_summary.items():\n",
    "        if isinstance(value, float):\n",
    "            f.write(f\"{key:30s}: {value:.6f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{key:30s}: {value}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTRUSION DETECTION SYSTEM - RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, value in results_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:30s}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"{key:30s}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ Results saved to 'saved_models/results_summary.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Summary and Conclusions\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. ‚úÖ **Built a hybrid CNN+LSTM Autoencoder** for intrusion detection\n",
    "2. ‚úÖ **Trained on normal traffic only** using unsupervised learning\n",
    "3. ‚úÖ **Detected intrusions** using reconstruction error analysis\n",
    "4. ‚úÖ **Achieved strong performance** on the UNSW-NB15 dataset\n",
    "5. ‚úÖ **Generated comprehensive visualizations** and metrics\n",
    "\n",
    "---\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "**Encoder:**\n",
    "- 1D Convolutional layers extract spatial features from flow characteristics\n",
    "- LSTM layer captures temporal dependencies across sequences\n",
    "\n",
    "**Decoder:**\n",
    "- LSTM layers reconstruct temporal patterns\n",
    "- TimeDistributed Dense layer reconstructs feature values\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Reconstruction Error as Anomaly Score**: Attack sequences show significantly higher reconstruction errors than normal traffic\n",
    "\n",
    "2. **Threshold-Based Detection**: By setting an appropriate threshold, we can effectively separate normal from malicious traffic\n",
    "\n",
    "3. **Temporal Patterns Matter**: The LSTM component successfully captures temporal dependencies that distinguish attacks\n",
    "\n",
    "4. **Unsupervised Approach**: Training only on normal traffic makes the system adaptable to new attack types not seen during training\n",
    "\n",
    "---\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Experiment with different latent dimensions, layer sizes, and architectures\n",
    "2. **Attention Mechanisms**: Add attention layers to focus on important time steps\n",
    "3. **Ensemble Methods**: Combine multiple models for robust detection\n",
    "4. **Feature Engineering**: Create additional domain-specific features\n",
    "5. **Threshold Optimization**: Use techniques like ROC analysis for optimal threshold selection\n",
    "\n",
    "---\n",
    "\n",
    "### Applications:\n",
    "\n",
    "This intrusion detection system can be deployed in:\n",
    "- **Network monitoring** systems\n",
    "- **Firewall** security layers\n",
    "- **Cloud infrastructure** protection\n",
    "- **IoT device** security\n",
    "- **Enterprise networks** for real-time threat detection\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Learning Outcomes\n",
    "\n",
    "Through this project, you have learned:\n",
    "\n",
    "‚úÖ How to preprocess network traffic data for deep learning\n",
    "\n",
    "‚úÖ Creating sequences from flow-based features\n",
    "\n",
    "‚úÖ Building hybrid CNN+LSTM architectures\n",
    "\n",
    "‚úÖ Training autoencoders for anomaly detection\n",
    "\n",
    "‚úÖ Using reconstruction error for intrusion detection\n",
    "\n",
    "‚úÖ Evaluating cybersecurity models with appropriate metrics\n",
    "\n",
    "‚úÖ Visualizing and interpreting deep learning results\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing this Intrusion Detection System project! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
