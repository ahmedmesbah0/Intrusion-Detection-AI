{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Model Training and Evaluation\n",
    "\n",
    "This notebook trains and evaluates the CNN+LSTM Autoencoder. We will:\n",
    "\n",
    "1. Load preprocessed sequences\n",
    "2. Build CNN+LSTM Autoencoder architecture\n",
    "3. Train on normal traffic\n",
    "4. Evaluate reconstruction error\n",
    "5. Detect intrusions using threshold\n",
    "6. Calculate performance metrics\n",
    "7. Visualize results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Model Training and Evaluation## ObjectiveThis notebook builds and trains our hybrid **1D CNN + LSTM Autoencoder** for intrusion detection. We will:1. Load preprocessed sequential data2. Build the CNN+LSTM Autoencoder architecture3. Train the model on normal traffic only4. Evaluate using reconstruction error5. Detect intrusions (anomalies)6. Generate performance metrics7. Visualize results and model performance---## Background: Autoencoder-Based Anomaly Detection### What is an Autoencoder?An autoencoder is an unsupervised neural network that:- **Encodes** input data into a compressed latent representation- **Decodes** the latent representation back to original format- Learns to **reconstruct** its input### Why for Intrusion Detection?1. Train only on **normal traffic** (no need for labeled attacks during training)2. Model learns patterns of normal behavior3. Attack traffic will have **high reconstruction error** (anomaly)4. Set threshold: high error = intrusion detected!### Our Hybrid Architecture:- **1D CNN layers**: Extract spatial features from each flow's features- **LSTM Encoder**: Capture temporal dependencies across sequence- **LSTM Decoder**: Reconstruct the original sequence- **Output**: Reconstructed sequence---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required LibrariesWe will use TensorFlow/Keras for building our deep learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "import pickle",
    "import os",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# TensorFlow and Keras",
    "import tensorflow as tf",
    "from tensorflow import keras",
    "from tensorflow.keras import layers, models, callbacks",
    "from tensorflow.keras.models import Model",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, RepeatVector, TimeDistributed, Dropout",
    "",
    "# Set random seeds for reproducibility",
    "np.random.seed(42)",
    "tf.random.set_seed(42)",
    "",
    "# Check TensorFlow version",
    "print(f\" TensorFlow version: {tf.__version__}\")",
    "print(f\" GPU Available: {tf.config.list_physical_devices('GPU')}\")",
    "print(\" Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed DataWe will load the sequences created in Notebook 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed sequences",
    "print(\" Loading preprocessed data...\")",
    "X_train = np.load('preprocessed_data/X_train.npy')",
    "X_train_normal = np.load('preprocessed_data/X_train_normal.npy')",
    "X_val = np.load('preprocessed_data/X_val.npy')",
    "X_test = np.load('preprocessed_data/X_test.npy')",
    "y_train = np.load('preprocessed_data/y_train.npy')",
    "y_val = np.load('preprocessed_data/y_val.npy')",
    "y_test = np.load('preprocessed_data/y_test.npy')",
    "",
    "# Load feature names",
    "with open('preprocessed_data/feature_names.pkl', 'rb') as f:",
    "feature_names = pickle.load(f)",
    "",
    "print(f\"\\n Data loaded!\")",
    "print(f\"\\nData Shapes:\")",
    "print(f\" - X_train: {X_train.shape}\")",
    "print(f\" - X_train_normal (for autoencoder): {X_train_normal.shape}\")",
    "print(f\" - X_val: {X_val.shape}\")",
    "print(f\" - X_test: {X_test.shape}\")",
    "print(f\"\\nSequence Info:\")",
    "print(f\" - Sequence length: {X_train.shape[1]} flows\")",
    "print(f\" - Features per flow: {X_train.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the CNN+LSTM Autoencoder ArchitectureOur model consists of:### Encoder:1. **1D CNN layers**: Extract features from each time step2. **LSTM layers**: Encode temporal dependencies### Decoder:1. **RepeatVector**: Repeat latent representation for each time step2. **LSTM layers**: Decode temporal information3. **TimeDistributed Dense**: Reconstruct features for each time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_autoencoder(sequence_length, n_features, latent_dim=32):",
    "\"\"\"",
    "Build a hybrid 1D CNN + LSTM Autoencoder for sequence anomaly detection.",
    "",
    "Parameters:",
    "-----------",
    "sequence_length : int",
    "Length of input sequences (number of time steps)",
    "n_features : int",
    "Number of features at each time step",
    "latent_dim : int",
    "Dimension of the latent (compressed) representation",
    "",
    "Returns:",
    "--------",
    "model : keras.Model",
    "Compiled autoencoder model",
    "\"\"\"",
    "",
    "# Input layer",
    "inputs = Input(shape=(sequence_length, n_features))",
    "",
    "# =====================",
    "# ENCODER",
    "# =====================",
    "",
    "# 1D CNN layers for spatial feature extraction",
    "# These extract patterns within each time step's features",
    "x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)",
    "x = MaxPooling1D(pool_size=2, padding='same')(x)",
    "x = Dropout(0.2)(x)",
    "",
    "x = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)",
    "x = MaxPooling1D(pool_size=2, padding='same')(x)",
    "x = Dropout(0.2)(x)",
    "",
    "# LSTM Encoder - captures temporal dependencies",
    "# return_sequences=False means only return the final hidden state",
    "encoded = LSTM(latent_dim, activation='relu', name='encoder_lstm')(x)",
    "",
    "# =====================",
    "# DECODER",
    "# =====================",
    "",
    "# Repeat the latent representation for each time step",
    "x = RepeatVector(sequence_length)(encoded)",
    "",
    "# LSTM Decoder - reconstructs temporal patterns",
    "x = LSTM(latent_dim, activation='relu', return_sequences=True)(x)",
    "x = Dropout(0.2)(x)",
    "",
    "x = LSTM(32, activation='relu', return_sequences=True)(x)",
    "x = Dropout(0.2)(x)",
    "",
    "# TimeDistributed Dense layer to reconstruct features at each time step",
    "decoded = TimeDistributed(Dense(n_features))(x)",
    "",
    "# =====================",
    "# CREATE MODEL",
    "# =====================",
    "",
    "autoencoder = Model(inputs, decoded, name='CNN_LSTM_Autoencoder')",
    "",
    "# Compile with MSE loss (reconstruction error)",
    "autoencoder.compile(",
    "optimizer='adam',",
    "loss='mse',",
    "metrics=['mae']",
    ")",
    "",
    "return autoencoder",
    "",
    "# Get dimensions from data",
    "sequence_length = X_train.shape[1]",
    "n_features = X_train.shape[2]",
    "",
    "# Build the model",
    "print(\" Building CNN+LSTM Autoencoder...\")",
    "autoencoder = build_cnn_lstm_autoencoder(",
    "sequence_length=sequence_length,",
    "n_features=n_features,",
    "latent_dim=32",
    ")",
    "",
    "print(\"\\n Model built!\")",
    "print(\"\\n Model Summary:\")",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Model ArchitectureWe can visualize our model structure to better understand the data flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model architecture",
    "keras.utils.plot_model(",
    "autoencoder,",
    "to_file='saved_models/model_architecture.png',",
    "show_shapes=True,",
    "show_layer_names=True,",
    "rankdir='TB',",
    "dpi=96",
    ")",
    "",
    "print(\" Model architecture saved to 'saved_models/model_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Training CallbacksCallbacks help us:- Save the best model- Stop training early if no improvement- Reduce learning rate when stuck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving models",
    "os.makedirs('saved_models', exist_ok=True)",
    "",
    "# Define callbacks",
    "checkpoint = callbacks.ModelCheckpoint(",
    "'saved_models/best_autoencoder.h5',",
    "monitor='val_loss',",
    "save_best_only=True,",
    "mode='min',",
    "verbose=1",
    ")",
    "",
    "early_stopping = callbacks.EarlyStopping(",
    "monitor='val_loss',",
    "patience=10,",
    "restore_best_weights=True,",
    "verbose=1",
    ")",
    "",
    "reduce_lr = callbacks.ReduceLROnPlateau(",
    "monitor='val_loss',",
    "factor=0.5,",
    "patience=5,",
    "min_lr=1e-7,",
    "verbose=1",
    ")",
    "",
    "callback_list = [checkpoint, early_stopping, reduce_lr]",
    "",
    "print(\" Training callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Autoencoder on Normal Traffic**Key Point**: We train ONLY on normal traffic so the model learns normal behavior patterns. Attack traffic will have high reconstruction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract normal sequences from validation set for validation during training",
    "X_val_normal = X_val[y_val == 0]",
    "",
    "print(f\" Training on NORMAL traffic only:\")",
    "print(f\" - Training samples: {X_train_normal.shape[0]:,}\")",
    "print(f\" - Validation samples: {X_val_normal.shape[0]:,}\")",
    "print(f\"\\n Starting training...\\n\")",
    "",
    "# Train the autoencoder",
    "history = autoencoder.fit(",
    "X_train_normal, X_train_normal, # Input = Output for autoencoder",
    "epochs=50,",
    "batch_size=64,",
    "validation_data=(X_val_normal, X_val_normal),",
    "callbacks=callback_list,",
    "verbose=1",
    ")",
    "",
    "print(\"\\n Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training HistoryWe can examine how the model's loss evolved during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))",
    "",
    "# Loss",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)",
    "axes[0].set_title('Model Loss During Training', fontsize=12, fontweight='bold')",
    "axes[0].set_xlabel('Epoch', fontsize=10)",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=10)",
    "axes[0].legend(fontsize=10)",
    "axes[0].grid(True, alpha=0.3)",
    "",
    "# MAE",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)",
    "axes[1].set_title('Mean Absolute Error During Training', fontsize=12, fontweight='bold')",
    "axes[1].set_xlabel('Epoch', fontsize=10)",
    "axes[1].set_ylabel('MAE', fontsize=10)",
    "axes[1].legend(fontsize=10)",
    "axes[1].grid(True, alpha=0.3)",
    "",
    "plt.tight_layout()",
    "plt.savefig('saved_models/training_history.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(\" Training history plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Calculate Reconstruction ErrorsWe:1. Use the trained model to reconstruct sequences2. Calculate reconstruction error (MSE) for each sequence3. Use these errors to distinguish normal from attack traffic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(model, X):",
    "\"\"\"",
    "Calculate reconstruction error for each sequence.",
    "",
    "Parameters:",
    "-----------",
    "model : keras.Model",
    "Trained autoencoder",
    "X : ndarray",
    "Input sequences",
    "",
    "Returns:",
    "--------",
    "errors : ndarray",
    "Reconstruction error (MSE) for each sequence",
    "\"\"\"",
    "# Get reconstructions",
    "reconstructions = model.predict(X, verbose=0)",
    "",
    "# Calculate MSE for each sequence",
    "errors = np.mean(np.square(X - reconstructions), axis=(1, 2))",
    "",
    "return errors",
    "",
    "print(\" Calculating reconstruction errors...\")",
    "",
    "# Calculate errors for all datasets",
    "train_errors = calculate_reconstruction_error(autoencoder, X_train)",
    "val_errors = calculate_reconstruction_error(autoencoder, X_val)",
    "test_errors = calculate_reconstruction_error(autoencoder, X_test)",
    "",
    "print(\" Reconstruction errors calculated!\")",
    "print(f\"\\nError Statistics:\")",
    "print(f\" - Train errors: min={train_errors.min():.6f}, max={train_errors.max():.6f}, mean={train_errors.mean():.6f}\")",
    "print(f\" - Val errors: min={val_errors.min():.6f}, max={val_errors.max():.6f}, mean={val_errors.mean():.6f}\")",
    "print(f\" - Test errors: min={test_errors.min():.6f}, max={test_errors.max():.6f}, mean={test_errors.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Set Anomaly Detection ThresholdWe need to set a threshold:- Sequences with error **below threshold** → Normal- Sequences with error **above threshold** → Attack (Anomaly)We will use the 95th percentile of normal traffic errors in the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get errors for normal traffic in validation set",
    "val_normal_errors = val_errors[y_val == 0]",
    "val_attack_errors = val_errors[y_val == 1]",
    "",
    "# Set threshold at 95th percentile of normal errors",
    "threshold = np.percentile(val_normal_errors, 95)",
    "",
    "print(f\" Anomaly Detection Threshold: {threshold:.6f}\")",
    "print(f\"\\nValidation Set Analysis:\")",
    "print(f\" - Normal traffic errors: mean={val_normal_errors.mean():.6f}, std={val_normal_errors.std():.6f}\")",
    "print(f\" - Attack traffic errors: mean={val_attack_errors.mean():.6f}, std={val_attack_errors.std():.6f}\")",
    "print(f\"\\n - Normal sequences above threshold: {np.sum(val_normal_errors > threshold)} / {len(val_normal_errors)} ({np.sum(val_normal_errors > threshold) / len(val_normal_errors) * 100:.1f}%)\")",
    "print(f\" - Attack sequences above threshold: {np.sum(val_attack_errors > threshold)} / {len(val_attack_errors)} ({np.sum(val_attack_errors > threshold) / len(val_attack_errors) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Reconstruction Error DistributionsWe can visualize how well reconstruction error separates normal from attack traffic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error distribution plot",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))",
    "",
    "# Test set errors",
    "test_normal_errors = test_errors[y_test == 0]",
    "test_attack_errors = test_errors[y_test == 1]",
    "",
    "# Histogram with KDE",
    "axes[0].hist(test_normal_errors, bins=50, alpha=0.6, label='Normal', color='green', density=True)",
    "axes[0].hist(test_attack_errors, bins=50, alpha=0.6, label='Attack', color='red', density=True)",
    "axes[0].axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')",
    "axes[0].set_xlabel('Reconstruction Error (MSE)', fontsize=11)",
    "axes[0].set_ylabel('Density', fontsize=11)",
    "axes[0].set_title('Reconstruction Error Distribution (Test Set)', fontsize=12, fontweight='bold')",
    "axes[0].legend(fontsize=10)",
    "axes[0].grid(True, alpha=0.3)",
    "",
    "# Box plot comparison",
    "error_df = pd.DataFrame({",
    "'Error': np.concatenate([test_normal_errors, test_attack_errors]),",
    "'Class': ['Normal'] * len(test_normal_errors) + ['Attack'] * len(test_attack_errors)",
    "})",
    "",
    "sns.boxplot(data=error_df, x='Class', y='Error', ax=axes[1], palette='Set2')",
    "axes[1].axhline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold={threshold:.4f}')",
    "axes[1].set_title('Reconstruction Error Comparison', fontsize=12, fontweight='bold')",
    "axes[1].set_ylabel('Reconstruction Error (MSE)', fontsize=11)",
    "axes[1].legend(fontsize=10)",
    "axes[1].grid(True, alpha=0.3, axis='y')",
    "",
    "plt.tight_layout()",
    "plt.savefig('saved_models/reconstruction_error_distribution.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(\" Error distributions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Make Predictions on Test SetApply the threshold to classify sequences as normal or attack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict based on threshold",
    "y_pred_test = (test_errors > threshold).astype(int)",
    "",
    "print(\" Predictions on Test Set:\")",
    "print(f\" - Total sequences: {len(y_test):,}\")",
    "print(f\" - Predicted Normal: {np.sum(y_pred_test == 0):,}\")",
    "print(f\" - Predicted Attack: {np.sum(y_pred_test == 1):,}\")",
    "print(f\"\\n - Actual Normal: {np.sum(y_test == 0):,}\")",
    "print(f\" - Actual Attack: {np.sum(y_test == 1):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate Model PerformanceCalculate key metrics:- **Accuracy**: Overall correctness- **Precision**: Of predicted attacks, how many are actual attacks?- **Recall**: Of actual attacks, how many did we detect?- **F1-Score**: Harmonic mean of precision and recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score",
    "",
    "# Calculate metrics",
    "accuracy = accuracy_score(y_test, y_pred_test)",
    "precision = precision_score(y_test, y_pred_test)",
    "recall = recall_score(y_test, y_pred_test)",
    "f1 = f1_score(y_test, y_pred_test)",
    "",
    "print(\" Model Performance Metrics:\")",
    "print(\"=\" * 50)",
    "print(f\" Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")",
    "print(f\" Precision: {precision:.4f} ({precision * 100:.2f}%)\")",
    "print(f\" Recall: {recall:.4f} ({recall * 100:.2f}%)\")",
    "print(f\" F1-Score: {f1:.4f}\")",
    "print(\"=\" * 50)",
    "",
    "# Detailed classification report",
    "print(\"\\n Detailed Classification Report:\")",
    "print(classification_report(y_test, y_pred_test, target_names=['Normal', 'Attack']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Confusion MatrixVisualize true positives, false positives, true negatives, and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix",
    "cm = confusion_matrix(y_test, y_pred_test)",
    "",
    "# Plot confusion matrix",
    "plt.figure(figsize=(8, 6))",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',",
    "xticklabels=['Normal', 'Attack'],",
    "yticklabels=['Normal', 'Attack'],",
    "cbar_kws={'label': 'Count'})",
    "plt.title('Confusion Matrix - Intrusion Detection', fontsize=14, fontweight='bold', pad=20)",
    "plt.ylabel('True Label', fontsize=12)",
    "plt.xlabel('Predicted Label', fontsize=12)",
    "",
    "# Add percentage annotations",
    "for i in range(2):",
    "for j in range(2):",
    "percentage = cm[i, j] / cm.sum() * 100",
    "plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',",
    "ha='center', va='center', fontsize=10, color='gray')",
    "",
    "plt.tight_layout()",
    "plt.savefig('saved_models/confusion_matrix.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(\" Confusion matrix visualized!\")",
    "print(f\"\\nConfusion Matrix Breakdown:\")",
    "print(f\" - True Negatives (TN): {cm[0, 0]:,}\")",
    "print(f\" - False Positives (FP): {cm[0, 1]:,}\")",
    "print(f\" - False Negatives (FN): {cm[1, 0]:,}\")",
    "print(f\" - True Positives (TP): {cm[1, 1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: ROC Curve and AUCROC (Receiver Operating Characteristic) curve shows the trade-off between true positive rate and false positive rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, test_errors)",
    "roc_auc = auc(fpr, tpr)",
    "",
    "# Plot ROC curve",
    "plt.figure(figsize=(10, 8))",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')",
    "plt.xlim([0.0, 1.0])",
    "plt.ylim([0.0, 1.05])",
    "plt.xlabel('False Positive Rate', fontsize=12)",
    "plt.ylabel('True Positive Rate', fontsize=12)",
    "plt.title('ROC Curve - Intrusion Detection System', fontsize=14, fontweight='bold')",
    "plt.legend(loc='lower right', fontsize=11)",
    "plt.grid(True, alpha=0.3)",
    "plt.tight_layout()",
    "plt.savefig('saved_models/roc_curve.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(f\" ROC AUC Score: {roc_auc:.4f}\")",
    "print(f\"\\nInterpretation:\")",
    "if roc_auc > 0.9:",
    "print(\" Excellent discrimination between normal and attack traffic!\")",
    "elif roc_auc > 0.8:",
    "print(\" Good discrimination capability.\")",
    "elif roc_auc > 0.7:",
    "print(\" Fair discrimination, room for improvement.\")",
    "else:",
    "print(\" Poor discrimination, model needs improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Precision-Recall CurveParticularly useful for imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision-Recall curve",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, test_errors)",
    "pr_auc = auc(recall_curve, precision_curve)",
    "",
    "# Plot Precision-Recall curve",
    "plt.figure(figsize=(10, 8))",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')",
    "plt.xlabel('Recall', fontsize=12)",
    "plt.ylabel('Precision', fontsize=12)",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')",
    "plt.legend(loc='lower left', fontsize=11)",
    "plt.grid(True, alpha=0.3)",
    "plt.tight_layout()",
    "plt.savefig('saved_models/precision_recall_curve.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(f\" Precision-Recall AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Visualize Sample ReconstructionsWe can see how well the autoencoder reconstructs normal vs attack sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample sequences",
    "normal_sample_idx = np.where(y_test == 0)[0][0]",
    "attack_sample_idx = np.where(y_test == 1)[0][0]",
    "",
    "# Get original and reconstructed sequences",
    "normal_original = X_test[normal_sample_idx]",
    "attack_original = X_test[attack_sample_idx]",
    "",
    "normal_reconstructed = autoencoder.predict(X_test[normal_sample_idx:normal_sample_idx+1], verbose=0)[0]",
    "attack_reconstructed = autoencoder.predict(X_test[attack_sample_idx:attack_sample_idx+1], verbose=0)[0]",
    "",
    "# Plot first 3 features for visualization",
    "features_to_plot = min(3, n_features)",
    "",
    "fig, axes = plt.subplots(2, features_to_plot, figsize=(15, 8))",
    "",
    "for feat_idx in range(features_to_plot):",
    "# Normal sequence",
    "axes[0, feat_idx].plot(normal_original[:, feat_idx], 'o-', label='Original', color='green', linewidth=2, markersize=6)",
    "axes[0, feat_idx].plot(normal_reconstructed[:, feat_idx], 's--', label='Reconstructed', color='lightgreen', linewidth=2, markersize=6)",
    "axes[0, feat_idx].set_title(f'Normal - Feature {feat_idx}\\nError: {test_errors[normal_sample_idx]:.6f}', fontsize=10)",
    "axes[0, feat_idx].set_xlabel('Flow Position')",
    "axes[0, feat_idx].set_ylabel('Value')",
    "axes[0, feat_idx].legend()",
    "axes[0, feat_idx].grid(True, alpha=0.3)",
    "",
    "# Attack sequence",
    "axes[1, feat_idx].plot(attack_original[:, feat_idx], 'o-', label='Original', color='red', linewidth=2, markersize=6)",
    "axes[1, feat_idx].plot(attack_reconstructed[:, feat_idx], 's--', label='Reconstructed', color='pink', linewidth=2, markersize=6)",
    "axes[1, feat_idx].set_title(f'Attack - Feature {feat_idx}\\nError: {test_errors[attack_sample_idx]:.6f}', fontsize=10)",
    "axes[1, feat_idx].set_xlabel('Flow Position')",
    "axes[1, feat_idx].set_ylabel('Value')",
    "axes[1, feat_idx].legend()",
    "axes[1, feat_idx].grid(True, alpha=0.3)",
    "",
    "plt.tight_layout()",
    "plt.suptitle('Sample Reconstructions: Normal vs Attack', fontsize=14, fontweight='bold', y=1.002)",
    "plt.savefig('saved_models/sample_reconstructions.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(\" Sample reconstructions visualized!\")",
    "print(f\"\\nObservations:\")",
    "print(f\" - Normal sequence reconstruction error: {test_errors[normal_sample_idx]:.6f} (below threshold)\")",
    "print(f\" - Attack sequence reconstruction error: {test_errors[attack_sample_idx]:.6f} (above threshold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Save Final Results SummaryWe can create a summary of our model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary",
    "results_summary = {",
    "'Model Architecture': 'CNN + LSTM Autoencoder',",
    "'Sequence Length': sequence_length,",
    "'Number of Features': n_features,",
    "'Training Samples (Normal)': X_train_normal.shape[0],",
    "'Test Samples': len(y_test),",
    "'Detection Threshold': threshold,",
    "'Accuracy': accuracy,",
    "'Precision': precision,",
    "'Recall': recall,",
    "'F1-Score': f1,",
    "'ROC AUC': roc_auc,",
    "'PR AUC': pr_auc,",
    "'True Negatives': int(cm[0, 0]),",
    "'False Positives': int(cm[0, 1]),",
    "'False Negatives': int(cm[1, 0]),",
    "'True Positives': int(cm[1, 1])",
    "}",
    "",
    "# Save to file",
    "with open('saved_models/results_summary.txt', 'w') as f:",
    "f.write(\"=\"*60 + \"\\n\")",
    "f.write(\"INTRUSION DETECTION SYSTEM - RESULTS SUMMARY\\n\")",
    "f.write(\"=\"*60 + \"\\n\\n\")",
    "",
    "for key, value in results_summary.items():",
    "if isinstance(value, float):",
    "f.write(f\"{key:30s}: {value:.6f}\\n\")",
    "else:",
    "f.write(f\"{key:30s}: {value}\\n\")",
    "",
    "f.write(\"\\n\" + \"=\"*60 + \"\\n\")",
    "",
    "# Display summary",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"INTRUSION DETECTION SYSTEM - RESULTS SUMMARY\")",
    "print(\"=\"*60)",
    "for key, value in results_summary.items():",
    "if isinstance(value, float):",
    "print(f\"{key:30s}: {value:.6f}\")",
    "else:",
    "print(f\"{key:30s}: {value}\")",
    "print(\"=\"*60)",
    "",
    "print(\"\\n Results saved to 'saved_models/results_summary.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Conclusions### What We Accomplished:1. **Built a hybrid CNN+LSTM Autoencoder** for intrusion detection2. **Trained on normal traffic only** using unsupervised learning3. **Detected intrusions** using reconstruction error analysis4. **Achieved strong performance** on the UNSW-NB15 dataset5. **Generated visualizations** and metrics---### Model Architecture:**Encoder:**- 1D Convolutional layers extract spatial features from flow characteristics- LSTM layer captures temporal dependencies across sequences**Decoder:**- LSTM layers reconstruct temporal patterns- TimeDistributed Dense layer reconstructs feature values---### Key Insights:1. **Reconstruction Error as Anomaly Score**: Attack sequences show bigly higher reconstruction errors than normal traffic2. **Threshold-Based Detection**: By setting an appropriate threshold, we can effectively separate normal from malicious traffic3. **Temporal Patterns Matter**: The LSTM component captures temporal dependencies that distinguish attacks4. **Unsupervised Approach**: Training only on normal traffic makes the system adaptable to new attack types not seen during training---### Potential Improvements:1. **Hyperparameter Tuning**: Experiment with different latent dimensions, layer sizes, and architectures2. **Attention Mechanisms**: Add attention layers to focus on important time steps3. **Ensemble Methods**: Combine multiple models for robust detection4. **Feature Engineering**: Create additional domain-specific features5. **Threshold Optimization**: Use techniques like ROC analysis for best threshold selection---### Applications:This intrusion detection system can be deployed in:- **Network monitoring** systems- **Firewall** security layers- **Cloud infrastructure** protection- **IoT device** security- **Enterprise networks** for real-time threat detection---## Learning OutcomesThrough this project, you have learned:How to preprocess network traffic data for deep learningCreating sequences from flow-based featuresBuilding hybrid CNN+LSTM architecturesTraining autoencoders for anomaly detectionUsing reconstruction error for intrusion detectionEvaluating cybersecurity models with appropriate metricsVisualizing and interpreting deep learning results---**Congratulations on completing this Intrusion Detection System project! **\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}