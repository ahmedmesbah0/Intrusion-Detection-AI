{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 1: Data Preprocessing\n",
                "\n",
                "## üéØ Objective\n",
                "\n",
                "This notebook handles the preprocessing of the UNSW-NB15 dataset for our intrusion detection system. We will:\n",
                "\n",
                "1. Load the raw dataset from CSV files\n",
                "2. Perform exploratory data inspection\n",
                "3. Clean and handle missing values\n",
                "4. Normalize numerical features\n",
                "5. Encode categorical features\n",
                "6. Create sequences of network flows for temporal modeling\n",
                "7. Split data into train, validation, and test sets\n",
                "8. Save preprocessed data for subsequent notebooks\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Background: UNSW-NB15 Dataset\n",
                "\n",
                "The UNSW-NB15 dataset is a modern network intrusion detection dataset that contains:\n",
                "- **49 features** describing network flow characteristics\n",
                "- **Normal traffic** and **9 attack categories**\n",
                "- Real-world network traffic patterns\n",
                "\n",
                "**Attack Categories:**\n",
                "- Fuzzers: Attempts to discover vulnerabilities by sending random data\n",
                "- DoS: Denial of Service attacks\n",
                "- Exploits: Exploitation of known vulnerabilities\n",
                "- Reconnaissance: Scanning and probing\n",
                "- Shellcode: Code injection attacks\n",
                "- Analysis, Backdoor, Generic, Worms\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Required Libraries\n",
                "\n",
                "We'll use:\n",
                "- **pandas**: For data manipulation\n",
                "- **numpy**: For numerical operations\n",
                "- **sklearn**: For preprocessing and data splitting\n",
                "- **pickle**: For saving processed data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import os\n",
                "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load the Dataset\n",
                "\n",
                "The UNSW-NB15 dataset typically comes in two files:\n",
                "- Training set\n",
                "- Testing set\n",
                "\n",
                "We'll load both and combine them for our preprocessing pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define data paths\n",
                "TRAIN_PATH = 'data/UNSW_NB15_training-set.csv'\n",
                "TEST_PATH = 'data/UNSW_NB15_testing-set.csv'\n",
                "\n",
                "# Load the datasets\n",
                "print(\"üìÇ Loading UNSW-NB15 dataset...\")\n",
                "train_df = pd.read_csv(TRAIN_PATH)\n",
                "test_df = pd.read_csv(TEST_PATH)\n",
                "\n",
                "# Combine both datasets for unified preprocessing\n",
                "df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
                "\n",
                "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
                "print(f\"   - Training set: {train_df.shape[0]} samples\")\n",
                "print(f\"   - Testing set: {test_df.shape[0]} samples\")\n",
                "print(f\"   - Combined dataset: {df.shape[0]} samples, {df.shape[1]} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Initial Data Inspection\n",
                "\n",
                "Let's examine the structure of our dataset to understand:\n",
                "- Feature names and types\n",
                "- Missing values\n",
                "- Basic statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first few rows\n",
                "print(\"üìä First 5 rows of the dataset:\")\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset information\n",
                "print(\"\\nüìã Dataset Information:\")\n",
                "print(df.info())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"\\nüîç Missing values per column:\")\n",
                "missing = df.isnull().sum()\n",
                "missing = missing[missing > 0].sort_values(ascending=False)\n",
                "if len(missing) > 0:\n",
                "    print(missing)\n",
                "else:\n",
                "    print(\"No missing values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check class distribution\n",
                "print(\"\\nüìä Class Distribution:\")\n",
                "if 'label' in df.columns:\n",
                "    print(df['label'].value_counts())\n",
                "    print(f\"\\nNormal traffic: {(df['label'] == 0).sum()} samples\")\n",
                "    print(f\"Attack traffic: {(df['label'] == 1).sum()} samples\")\n",
                "    print(f\"Attack ratio: {(df['label'] == 1).sum() / len(df) * 100:.2f}%\")\n",
                "\n",
                "if 'attack_cat' in df.columns:\n",
                "    print(\"\\nüéØ Attack Categories:\")\n",
                "    print(df['attack_cat'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Feature Selection and Cleaning\n",
                "\n",
                "We'll:\n",
                "1. Identify and separate feature types (numerical vs categorical)\n",
                "2. Remove irrelevant features (IDs, timestamps that don't add value)\n",
                "3. Handle missing values\n",
                "4. Store labels for later use"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store labels separately\n",
                "labels = df['label'].values if 'label' in df.columns else None\n",
                "attack_categories = df['attack_cat'].values if 'attack_cat' in df.columns else None\n",
                "\n",
                "# Features to drop (non-predictive or identifier columns)\n",
                "# Adjust these based on your specific dataset columns\n",
                "columns_to_drop = ['id', 'label', 'attack_cat']\n",
                "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
                "\n",
                "# Create feature dataframe\n",
                "features_df = df.drop(columns=columns_to_drop, errors='ignore')\n",
                "\n",
                "print(f\"‚úÖ Selected {features_df.shape[1]} features for modeling\")\n",
                "print(f\"\\nFeature columns: {list(features_df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify categorical and numerical columns\n",
                "categorical_cols = features_df.select_dtypes(include=['object']).columns.tolist()\n",
                "numerical_cols = features_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "\n",
                "print(f\"üî¢ Numerical features: {len(numerical_cols)}\")\n",
                "print(f\"üìù Categorical features: {len(categorical_cols)}\")\n",
                "print(f\"\\nCategorical columns: {categorical_cols}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Handle Missing Values\n",
                "\n",
                "For missing values:\n",
                "- **Numerical features**: Fill with median\n",
                "- **Categorical features**: Fill with mode or 'unknown'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle missing values in numerical columns\n",
                "for col in numerical_cols:\n",
                "    if features_df[col].isnull().sum() > 0:\n",
                "        median_value = features_df[col].median()\n",
                "        features_df[col].fillna(median_value, inplace=True)\n",
                "        print(f\"Filled {col} with median: {median_value}\")\n",
                "\n",
                "# Handle missing values in categorical columns\n",
                "for col in categorical_cols:\n",
                "    if features_df[col].isnull().sum() > 0:\n",
                "        features_df[col].fillna('unknown', inplace=True)\n",
                "        print(f\"Filled {col} with 'unknown'\")\n",
                "\n",
                "print(\"\\n‚úÖ Missing values handled!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Encode Categorical Features\n",
                "\n",
                "We need to convert categorical variables (like protocol type, service, state) into numerical values using Label Encoding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize label encoders dictionary to save for later use\n",
                "label_encoders = {}\n",
                "\n",
                "# Encode categorical columns\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    features_df[col] = le.fit_transform(features_df[col].astype(str))\n",
                "    label_encoders[col] = le\n",
                "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
                "\n",
                "print(\"\\n‚úÖ Categorical features encoded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Normalize Numerical Features\n",
                "\n",
                "Neural networks perform better with normalized data. We'll use MinMaxScaler to scale all features to the range [0, 1]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and fit the scaler\n",
                "scaler = MinMaxScaler()\n",
                "features_scaled = scaler.fit_transform(features_df)\n",
                "\n",
                "# Convert back to DataFrame for easier handling\n",
                "features_scaled_df = pd.DataFrame(features_scaled, columns=features_df.columns)\n",
                "\n",
                "print(f\"‚úÖ Features normalized to range [0, 1]\")\n",
                "print(f\"\\nScaled features shape: {features_scaled_df.shape}\")\n",
                "print(f\"\\nSample statistics after scaling:\")\n",
                "print(features_scaled_df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Create Sequences for Temporal Modeling\n",
                "\n",
                "For our CNN+LSTM autoencoder, we need to create sequences of network flows. \n",
                "\n",
                "**Why sequences?**\n",
                "- Network attacks often span multiple packets/flows\n",
                "- Temporal patterns help identify anomalies\n",
                "- LSTM can learn time dependencies\n",
                "\n",
                "We'll use a sliding window approach to create fixed-length sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(data, labels, sequence_length=10, stride=5):\n",
                "    \"\"\"\n",
                "    Create sequences from the dataset using a sliding window.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    data : array-like\n",
                "        Feature data (samples x features)\n",
                "    labels : array-like\n",
                "        Binary labels (0=normal, 1=attack)\n",
                "    sequence_length : int\n",
                "        Length of each sequence (number of flows)\n",
                "    stride : int\n",
                "        Step size for sliding window\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    sequences : ndarray\n",
                "        Sequences of shape (num_sequences, sequence_length, num_features)\n",
                "    sequence_labels : ndarray\n",
                "        Labels for each sequence (1 if any flow in sequence is attack)\n",
                "    \"\"\"\n",
                "    sequences = []\n",
                "    sequence_labels = []\n",
                "    \n",
                "    # Sliding window to create sequences\n",
                "    for i in range(0, len(data) - sequence_length + 1, stride):\n",
                "        # Extract sequence\n",
                "        seq = data[i:i + sequence_length]\n",
                "        \n",
                "        # Label is 1 if any flow in the sequence is an attack\n",
                "        label = labels[i:i + sequence_length]\n",
                "        seq_label = 1 if np.any(label == 1) else 0\n",
                "        \n",
                "        sequences.append(seq)\n",
                "        sequence_labels.append(seq_label)\n",
                "    \n",
                "    return np.array(sequences), np.array(sequence_labels)\n",
                "\n",
                "# Set sequence parameters\n",
                "SEQUENCE_LENGTH = 10  # Each sequence contains 10 network flows\n",
                "STRIDE = 5            # Move 5 flows forward for next sequence\n",
                "\n",
                "print(f\"Creating sequences with length={SEQUENCE_LENGTH}, stride={STRIDE}...\")\n",
                "\n",
                "# Create sequences\n",
                "X_sequences, y_sequences = create_sequences(\n",
                "    features_scaled_df.values, \n",
                "    labels, \n",
                "    sequence_length=SEQUENCE_LENGTH,\n",
                "    stride=STRIDE\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ Sequences created!\")\n",
                "print(f\"   - Number of sequences: {X_sequences.shape[0]}\")\n",
                "print(f\"   - Sequence shape: {X_sequences.shape[1:]}\")\n",
                "print(f\"   - Normal sequences: {np.sum(y_sequences == 0)}\")\n",
                "print(f\"   - Attack sequences: {np.sum(y_sequences == 1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Split Data into Train, Validation, and Test Sets\n",
                "\n",
                "We'll split the data:\n",
                "- **70%** Training set (for model training)\n",
                "- **15%** Validation set (for hyperparameter tuning)\n",
                "- **15%** Test set (for final evaluation)\n",
                "\n",
                "**Important**: For autoencoder training, we'll primarily use normal traffic in the training set!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First split: 70% train, 30% temp (for validation and test)\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X_sequences, y_sequences, \n",
                "    test_size=0.3, \n",
                "    random_state=42,\n",
                "    stratify=y_sequences  # Maintain class distribution\n",
                ")\n",
                "\n",
                "# Second split: Split temp into 50% validation, 50% test (15% each of total)\n",
                "X_val, X_test, y_val, y_test = train_test_split(\n",
                "    X_temp, y_temp, \n",
                "    test_size=0.5, \n",
                "    random_state=42,\n",
                "    stratify=y_temp\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Data split completed!\")\n",
                "print(f\"\\nTraining set:\")\n",
                "print(f\"   - Shape: {X_train.shape}\")\n",
                "print(f\"   - Normal: {np.sum(y_train == 0)}, Attack: {np.sum(y_train == 1)}\")\n",
                "\n",
                "print(f\"\\nValidation set:\")\n",
                "print(f\"   - Shape: {X_val.shape}\")\n",
                "print(f\"   - Normal: {np.sum(y_val == 0)}, Attack: {np.sum(y_val == 1)}\")\n",
                "\n",
                "print(f\"\\nTest set:\")\n",
                "print(f\"   - Shape: {X_test.shape}\")\n",
                "print(f\"   - Normal: {np.sum(y_test == 0)}, Attack: {np.sum(y_test == 1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Extract Normal Traffic for Autoencoder Training\n",
                "\n",
                "Autoencoders learn to reconstruct normal patterns. We train only on normal traffic, then use reconstruction error to detect anomalies (attacks)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract only normal traffic for training the autoencoder\n",
                "X_train_normal = X_train[y_train == 0]\n",
                "\n",
                "print(f\"‚úÖ Extracted normal traffic for autoencoder training\")\n",
                "print(f\"   - Normal training sequences: {X_train_normal.shape[0]}\")\n",
                "print(f\"   - This represents {X_train_normal.shape[0] / X_train.shape[0] * 100:.1f}% of training data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Save Preprocessed Data\n",
                "\n",
                "We'll save all preprocessed data for use in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "os.makedirs('preprocessed_data', exist_ok=True)\n",
                "\n",
                "# Save data as numpy arrays\n",
                "np.save('preprocessed_data/X_train.npy', X_train)\n",
                "np.save('preprocessed_data/X_train_normal.npy', X_train_normal)\n",
                "np.save('preprocessed_data/X_val.npy', X_val)\n",
                "np.save('preprocessed_data/X_test.npy', X_test)\n",
                "np.save('preprocessed_data/y_train.npy', y_train)\n",
                "np.save('preprocessed_data/y_val.npy', y_val)\n",
                "np.save('preprocessed_data/y_test.npy', y_test)\n",
                "\n",
                "# Save preprocessing objects\n",
                "with open('preprocessed_data/scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler, f)\n",
                "\n",
                "with open('preprocessed_data/label_encoders.pkl', 'wb') as f:\n",
                "    pickle.dump(label_encoders, f)\n",
                "\n",
                "# Save feature names\n",
                "with open('preprocessed_data/feature_names.pkl', 'wb') as f:\n",
                "    pickle.dump(list(features_df.columns), f)\n",
                "\n",
                "print(\"‚úÖ All preprocessed data saved successfully!\")\n",
                "print(\"\\nüìÅ Saved files:\")\n",
                "print(\"   - X_train.npy, X_train_normal.npy, X_val.npy, X_test.npy\")\n",
                "print(\"   - y_train.npy, y_val.npy, y_test.npy\")\n",
                "print(\"   - scaler.pkl, label_encoders.pkl, feature_names.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Summary\n",
                "\n",
                "In this notebook, we successfully:\n",
                "\n",
                "1. ‚úÖ Loaded the UNSW-NB15 dataset\n",
                "2. ‚úÖ Inspected and cleaned the data\n",
                "3. ‚úÖ Handled missing values\n",
                "4. ‚úÖ Encoded categorical features\n",
                "5. ‚úÖ Normalized all features to [0, 1] range\n",
                "6. ‚úÖ Created sequences for temporal modeling\n",
                "7. ‚úÖ Split data into train/validation/test sets\n",
                "8. ‚úÖ Extracted normal traffic for autoencoder training\n",
                "9. ‚úÖ Saved all preprocessed data\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Next Steps\n",
                "\n",
                "Proceed to **Notebook 2: Visualization** to:\n",
                "- Explore feature distributions\n",
                "- Visualize attack patterns\n",
                "- Understand correlations\n",
                "- Gain insights into the dataset\n",
                "\n",
                "---\n",
                "\n",
                "**Note**: Make sure the `preprocessed_data/` directory contains all saved files before proceeding to the next notebook!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}