{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Preprocessing\n",
    "\n",
    "This notebook handles preprocessing of the UNSW-NB15 dataset. We will:\n",
    "\n",
    "1. Load raw dataset from CSV files\n",
    "2. Perform data inspection\n",
    "3. Clean and handle missing values\n",
    "4. Normalize numerical features\n",
    "5. Encode categorical features\n",
    "6. Create sequences for temporal modeling\n",
    "7. Split into train/validation/test sets\n",
    "8. Save preprocessed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required LibrariesUsing:- **pandas**: For data manipulation- **numpy**: For numerical operations- **sklearn**: For preprocessing and data splitting- **pickle**: For saving processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported complete"
     ]
    }
   ],
   "source": [
    "import pandas as pd",
    "import numpy as np",
    "import pickle",
    "import os",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder",
    "from sklearn.model_selection import train_test_split",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# Set random seed for reproducibility",
    "np.random.seed(42)",
    "",
    "print(\" Libraries imported complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the DatasetThe UNSW-NB15 dataset typically comes in two files:- Training set- Testing setWe will load both and combine them for our preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading UNSW-NB15 dataset...",
      "",
      "Dataset loaded complete",
      "- Training set: 82332 samples",
      "- Testing set: 82332 samples",
      "- Combined dataset: 164664 samples, 45 features"
     ]
    }
   ],
   "source": [
    "# Define data paths",
    "TRAIN_PATH = r'/home/mesbah7/Github/Repos/Intrusion-Detection-AI/dataset_kaggle/UNSW_NB15_training-set.csv'",
    "TEST_PATH = r'/home/mesbah7/Github/Repos/Intrusion-Detection-AI/dataset_kaggle/UNSW_NB15_training-set.csv'",
    "",
    "# Load the datasets",
    "print(\" Loading UNSW-NB15 dataset...\")",
    "train_df = pd.read_csv(TRAIN_PATH)",
    "test_df = pd.read_csv(TEST_PATH)",
    "",
    "# Combine both datasets for unified preprocessing",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=True)",
    "",
    "print(f\"\\n Dataset loaded complete\")",
    "print(f\" - Training set: {train_df.shape[0]} samples\")",
    "print(f\" - Testing set: {test_df.shape[0]} samples\")",
    "print(f\" - Combined dataset: {df.shape[0]} samples, {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initial Data InspectionLet us examine the structure of our dataset to understand:- Feature names and types- Missing values- Basic statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>90909.0902</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1762</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.0003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.0051</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>166666.6608</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "      <td>100000.0025</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
       "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
       "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
       "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
       "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
       "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
       "\n",
       "          rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "0   90909.0902  ...                 1               2             0   \n",
       "1  125000.0003  ...                 1               2             0   \n",
       "2  200000.0051  ...                 1               3             0   \n",
       "3  166666.6608  ...                 1               3             0   \n",
       "4  100000.0025  ...                 1               3             0   \n",
       "\n",
       "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
       "0           0                 0           1           2                0   \n",
       "1           0                 0           1           2                0   \n",
       "2           0                 0           1           3                0   \n",
       "3           0                 0           2           3                0   \n",
       "4           0                 0           2           3                0   \n",
       "\n",
       "   attack_cat  label  \n",
       "0      Normal      0  \n",
       "1      Normal      0  \n",
       "2      Normal      0  \n",
       "3      Normal      0  \n",
       "4      Normal      0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows",
    "print(\" First 5 rows of the dataset:\")",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
       "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
       "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
       "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
       "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
       "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
       "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
       "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>90909.09020</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1762</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.00030</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.00510</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>166666.66080</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "      <td>100000.00250</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>333333.32150</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>166666.66080</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1384</td>\n",
       "      <td>0</td>\n",
       "      <td>35714.28522</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>arp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>arp</td>\n",
       "      <td>-</td>\n",
       "      <td>INT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
       "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
       "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
       "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
       "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
       "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
       "5   6  0.000003   udp       -   INT      2      0     784       0   \n",
       "6   7  0.000006   udp       -   INT      2      0    1960       0   \n",
       "7   8  0.000028   udp       -   INT      2      0    1384       0   \n",
       "8   9  0.000000   arp       -   INT      1      0      46       0   \n",
       "9  10  0.000000   arp       -   INT      1      0      46       0   \n",
       "\n",
       "           rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "0   90909.09020  ...                 1               2             0   \n",
       "1  125000.00030  ...                 1               2             0   \n",
       "2  200000.00510  ...                 1               3             0   \n",
       "3  166666.66080  ...                 1               3             0   \n",
       "4  100000.00250  ...                 1               3             0   \n",
       "5  333333.32150  ...                 1               2             0   \n",
       "6  166666.66080  ...                 1               2             0   \n",
       "7   35714.28522  ...                 1               3             0   \n",
       "8       0.00000  ...                 2               2             0   \n",
       "9       0.00000  ...                 2               2             0   \n",
       "\n",
       "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
       "0           0                 0           1           2                0   \n",
       "1           0                 0           1           2                0   \n",
       "2           0                 0           1           3                0   \n",
       "3           0                 0           2           3                0   \n",
       "4           0                 0           2           3                0   \n",
       "5           0                 0           2           2                0   \n",
       "6           0                 0           2           2                0   \n",
       "7           0                 0           1           3                0   \n",
       "8           0                 0           2           2                1   \n",
       "9           0                 0           2           2                1   \n",
       "\n",
       "   attack_cat  label  \n",
       "0      Normal      0  \n",
       "1      Normal      0  \n",
       "2      Normal      0  \n",
       "3      Normal      0  \n",
       "4      Normal      0  \n",
       "5      Normal      0  \n",
       "6      Normal      0  \n",
       "7      Normal      0  \n",
       "8      Normal      0  \n",
       "9      Normal      0  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service\n",
       "-           94306\n",
       "dns         42734\n",
       "http        16574\n",
       "smtp         3702\n",
       "ftp          3104\n",
       "ftp-data     2792\n",
       "pop3          846\n",
       "ssh           408\n",
       "ssl            60\n",
       "snmp           58\n",
       "dhcp           52\n",
       "radius         18\n",
       "irc            10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['service'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service\n",
       "dns         137040\n",
       "http         16574\n",
       "smtp          3702\n",
       "ftp           3104\n",
       "ftp-data      2792\n",
       "pop3           846\n",
       "ssh            408\n",
       "ssl             60\n",
       "snmp            58\n",
       "dhcp            52\n",
       "radius          18\n",
       "irc             10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent = df[df['service'] != '-']['service'].mode()[0]",
    "df['service'] = df['service'].replace('-', most_frequent)",
    "# In your case, this would replace all '-' with 'dns'",
    "df['service'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "",
      "üîç Missing values per column:",
      "No missing values found!"
     ]
    }
   ],
   "source": [
    "# Check for missing values",
    "print(\"\\n Missing values per column:\")",
    "missing = df.isnull().sum()",
    "missing = missing[missing > 0].sort_values(ascending=False)",
    "if len(missing) > 0:",
    "print(missing)",
    "else:",
    "print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "",
      "Class Distribution:",
      "label",
      "1 90664",
      "0 74000",
      "Name: count, dtype: int64",
      "",
      "Normal traffic: 74000 samples",
      "Attack traffic: 90664 samples",
      "Attack ratio: 55.06%",
      "",
      "Attack Categories:",
      "attack_cat",
      "Normal 74000",
      "Generic 37742",
      "Exploits 22264",
      "Fuzzers 12124",
      "DoS 8178",
      "Reconnaissance 6992",
      "Analysis 1354",
      "Backdoor 1166",
      "Shellcode 756",
      "Worms 88",
      "Name: count, dtype: int64"
     ]
    }
   ],
   "source": [
    "# Check class distribution",
    "print(\"\\n Class Distribution:\")",
    "if 'label' in df.columns:",
    "print(df['label'].value_counts())",
    "print(f\"\\nNormal traffic: {(df['label'] == 0).sum()} samples\")",
    "print(f\"Attack traffic: {(df['label'] == 1).sum()} samples\")",
    "print(f\"Attack ratio: {(df['label'] == 1).sum() / len(df) * 100:.2f}%\")",
    "",
    "if 'attack_cat' in df.columns:",
    "print(\"\\n Attack Categories:\")",
    "print(df['attack_cat'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Selection and CleaningWe will:1. Identify and separate feature types (numerical vs categorical)2. Remove irrelevant features (IDs, timestamps that don't add value)3. Handle missing values4. Store labels for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 42 features for modeling",
      "",
      "Feature columns: ['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']"
     ]
    }
   ],
   "source": [
    "# Store labels separately",
    "labels = df['label'].values if 'label' in df.columns else None",
    "attack_categories = df['attack_cat'].values if 'attack_cat' in df.columns else None",
    "",
    "# Features to drop (non-predictive or identifier columns)",
    "# Adjust these based on your specific dataset columns",
    "columns_to_drop = ['id', 'label', 'attack_cat']",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]",
    "",
    "# Create feature dataframe",
    "features_df = df.drop(columns=columns_to_drop, errors='ignore')",
    "",
    "print(f\" Selected {features_df.shape[1]} features for modeling\")",
    "print(f\"\\nFeature columns: {list(features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Numerical features: 39",
      "Categorical features: 3",
      "",
      "Categorical columns: ['proto', 'service', 'state']"
     ]
    }
   ],
   "source": [
    "# Identify categorical and numerical columns",
    "categorical_cols = features_df.select_dtypes(include=['object']).columns.tolist()",
    "numerical_cols = features_df.select_dtypes(include=['int64', 'float64']).columns.tolist()",
    "",
    "print(f\" Numerical features: {len(numerical_cols)}\")",
    "print(f\" Categorical features: {len(categorical_cols)}\")",
    "print(f\"\\nCategorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Handle Missing ValuesFor missing values:- **Numerical features**: Fill with median- **Categorical features**: Fill with mode or 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "",
      "Missing values handled!"
     ]
    }
   ],
   "source": [
    "# Handle missing values in numerical columns",
    "for col in numerical_cols:",
    "if features_df[col].isnull().sum() > 0:",
    "median_value = features_df[col].median()",
    "features_df[col].fillna(median_value, inplace=True)",
    "print(f\"Filled {col} with median: {median_value}\")",
    "",
    "# Handle missing values in categorical columns",
    "for col in categorical_cols:",
    "if features_df[col].isnull().sum() > 0:",
    "features_df[col].fillna('unknown', inplace=True)",
    "print(f\"Filled {col} with 'unknown'\")",
    "",
    "print(\"\\n Missing values handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Encode Categorical FeaturesWe need to convert categorical variables (like protocol type, service, state) into numerical values using Label Encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded proto: 131 unique values",
      "Encoded service: 12 unique values",
      "Encoded state: 7 unique values",
      "",
      "Categorical features encoded!"
     ]
    }
   ],
   "source": [
    "# Initialize label encoders dictionary to save for later use",
    "label_encoders = {}",
    "",
    "# Encode categorical columns",
    "for col in categorical_cols:",
    "le = LabelEncoder()",
    "features_df[col] = le.fit_transform(features_df[col].astype(str))",
    "label_encoders[col] = le",
    "print(f\"Encoded {col}: {len(le.classes_)} unique values\")",
    "",
    "print(\"\\n Categorical features encoded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Normalize Numerical FeaturesNeural networks perform better with normalized data. We will use MinMaxScaler to scale all features to the range [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized to range [0, 1]",
      "",
      "Scaled features shape: (164664, 42)",
      "",
      "Sample statistics after scaling:",
      "dur proto service state \\",
      "count 1.646640e+05 164664.000000 164664.000000 164664.000000",
      "mean 1.677927e-02 0.841141 0.142466 0.562459",
      "std 7.850718e-02 0.143363 0.133791 0.111728",
      "min 0.000000e+00 0.000000 0.000000 0.000000",
      "25% 1.333334e-07 0.853846 0.090909 0.500000",
      "50% 2.356334e-04 0.853846 0.090909 0.500000",
      "75% 1.198934e-02 0.900000 0.090909 0.666667",
      "max 1.000000e+00 1.000000 1.000000 1.000000",
      "",
      "spkts dpkts sbytes dbytes \\",
      "count 164664.000000 164664.000000 164664.000000 164664.000000",
      "mean 0.001660 0.001592 0.000555 0.000903",
      "std 0.012580 0.010490 0.011956 0.010334",
      "min 0.000000 0.000000 0.000000 0.000000",
      "25% 0.000094 0.000000 0.000006 0.000000",
      "50% 0.000470 0.000182 0.000036 0.000012",
      "75% 0.001033 0.000908 0.000087 0.000065",
      "max 1.000000 1.000000 1.000000 1.000000",
      "",
      "rate sttl ... ct_dst_ltm ct_src_dport_ltm \\",
      "count 164664.000000 164664.000000 ... 164664.000000 164664.000000",
      "mean 0.082411 0.709677 ... 0.081809 0.067740",
      "std 0.148620 0.398090 ... 0.145139 0.144647",
      "min 0.000000 0.000000 ... 0.000000 0.000000",
      "25% 0.000029 0.243137 ... 0.000000 0.000000",
      "50% 0.002650 0.996078 ... 0.017241 0.000000",
      "75% 0.111111 0.996078 ... 0.086207 0.051724",
      "max 1.000000 1.000000 ... 1.000000 1.000000",
      "",
      "ct_dst_sport_ltm ct_dst_src_ltm is_ftp_login ct_ftp_cmd \\",
      "count 164664.000000 164664.000000 164664.000000 164664.000000",
      "mean 0.071973 0.104135 0.004142 0.004190",
      "std 0.159875 0.184115 0.045585 0.046242",
      "min 0.000000 0.000000 0.000000 0.000000",
      "25% 0.000000 0.000000 0.000000 0.000000",
      "50% 0.000000 0.032258 0.000000 0.000000",
      "75% 0.054054 0.080645 0.000000 0.000000",
      "max 1.000000 1.000000 1.000000 1.000000",
      "",
      "ct_flw_http_mthd ct_src_ltm ct_srv_dst is_sm_ips_ports",
      "count 164664.000000 164664.000000 164664.000000 164664.000000",
      "mean 0.008109 0.092684 0.133840 0.011126",
      "std 0.039918 0.144812 0.182318 0.104890",
      "min 0.000000 0.000000 0.000000 0.000000",
      "25% 0.000000 0.000000 0.016393 0.000000",
      "50% 0.000000 0.033898 0.065574 0.000000",
      "75% 0.000000 0.101695 0.163934 0.000000",
      "max 1.000000 1.000000 1.000000 1.000000",
      "",
      "[8 rows x 42 columns]"
     ]
    }
   ],
   "source": [
    "# Create and fit the scaler",
    "scaler = MinMaxScaler()",
    "features_scaled = scaler.fit_transform(features_df)",
    "",
    "# Convert back to DataFrame for easier handling",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features_df.columns)",
    "",
    "print(f\" Features normalized to range [0, 1]\")",
    "print(f\"\\nScaled features shape: {features_scaled_df.shape}\")",
    "print(f\"\\nSample statistics after scaling:\")",
    "print(features_scaled_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Sequences for Temporal ModelingFor our CNN+LSTM autoencoder, we need to create sequences of network flows.**Why sequences?**- Network attacks often span multiple packets/flows- Temporal patterns help identify anomalies- LSTM can learn time dependenciesWe will use a sliding window approach to create fixed-length sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequences with length=10, stride=5...",
      "",
      "Sequences created!",
      "- Number of sequences: 32931",
      "- Sequence shape: (10, 42)",
      "- Normal sequences: 14791",
      "- Attack sequences: 18140"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, labels, sequence_length=10, stride=5):",
    "\"\"\"",
    "Create sequences from the dataset using a sliding window.",
    "",
    "Parameters:",
    "-----------",
    "data : array-like",
    "Feature data (samples x features)",
    "labels : array-like",
    "Binary labels (0=normal, 1=attack)",
    "sequence_length : int",
    "Length of each sequence (number of flows)",
    "stride : int",
    "Step size for sliding window",
    "",
    "Returns:",
    "--------",
    "sequences : ndarray",
    "Sequences of shape (num_sequences, sequence_length, num_features)",
    "sequence_labels : ndarray",
    "Labels for each sequence (1 if any flow in sequence is attack)",
    "\"\"\"",
    "sequences = []",
    "sequence_labels = []",
    "",
    "# Sliding window to create sequences",
    "for i in range(0, len(data) - sequence_length + 1, stride):",
    "# Extract sequence",
    "seq = data[i:i + sequence_length]",
    "",
    "# Label is 1 if any flow in the sequence is an attack",
    "label = labels[i:i + sequence_length]",
    "seq_label = 1 if np.any(label == 1) else 0",
    "",
    "sequences.append(seq)",
    "sequence_labels.append(seq_label)",
    "",
    "return np.array(sequences), np.array(sequence_labels)",
    "",
    "# Set sequence parameters",
    "SEQUENCE_LENGTH = 10 # Each sequence contains 10 network flows",
    "STRIDE = 5 # Move 5 flows forward for next sequence",
    "",
    "print(f\"Creating sequences with length={SEQUENCE_LENGTH}, stride={STRIDE}...\")",
    "",
    "# Create sequences",
    "X_sequences, y_sequences = create_sequences(",
    "features_scaled_df.values,",
    "labels,",
    "sequence_length=SEQUENCE_LENGTH,",
    "stride=STRIDE",
    ")",
    "",
    "print(f\"\\n Sequences created!\")",
    "print(f\" - Number of sequences: {X_sequences.shape[0]}\")",
    "print(f\" - Sequence shape: {X_sequences.shape[1:]}\")",
    "print(f\" - Normal sequences: {np.sum(y_sequences == 0)}\")",
    "print(f\" - Attack sequences: {np.sum(y_sequences == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Split Data into Train, Validation, and Test SetsWe will split the data:- **70%** Training set (for model training)- **15%** Validation set (for hyperparameter tuning)- **15%** Test set (for final evaluation)**Important**: For autoencoder training, we'll primarily use normal traffic in the training set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed!",
      "",
      "Training set:",
      "- Shape: (23051, 10, 42)",
      "- Normal: 10353, Attack: 12698",
      "",
      "Validation set:",
      "- Shape: (4940, 10, 42)",
      "- Normal: 2219, Attack: 2721",
      "",
      "Test set:",
      "- Shape: (4940, 10, 42)",
      "- Normal: 2219, Attack: 2721"
     ]
    }
   ],
   "source": [
    "# First split: 70% train, 30% temp (for validation and test)",
    "X_train, X_temp, y_train, y_temp = train_test_split(",
    "X_sequences, y_sequences,",
    "test_size=0.3,",
    "random_state=42,",
    "stratify=y_sequences # Maintain class distribution",
    ")",
    "",
    "# Second split: Split temp into 50% validation, 50% test (15% each of total)",
    "X_val, X_test, y_val, y_test = train_test_split(",
    "X_temp, y_temp,",
    "test_size=0.5,",
    "random_state=42,",
    "stratify=y_temp",
    ")",
    "",
    "print(\" Data split completed!\")",
    "print(f\"\\nTraining set:\")",
    "print(f\" - Shape: {X_train.shape}\")",
    "print(f\" - Normal: {np.sum(y_train == 0)}, Attack: {np.sum(y_train == 1)}\")",
    "",
    "print(f\"\\nValidation set:\")",
    "print(f\" - Shape: {X_val.shape}\")",
    "print(f\" - Normal: {np.sum(y_val == 0)}, Attack: {np.sum(y_val == 1)}\")",
    "",
    "print(f\"\\nTest set:\")",
    "print(f\" - Shape: {X_test.shape}\")",
    "print(f\" - Normal: {np.sum(y_test == 0)}, Attack: {np.sum(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Extract Normal Traffic for Autoencoder TrainingAutoencoders learn to reconstruct normal patterns. We train only on normal traffic, then use reconstruction error to detect anomalies (attacks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted normal traffic for autoencoder training",
      "- Normal training sequences: 10353",
      "- This represents 44.9% of training data"
     ]
    }
   ],
   "source": [
    "# Extract only normal traffic for training the autoencoder",
    "X_train_normal = X_train[y_train == 0]",
    "",
    "print(f\" Extracted normal traffic for autoencoder training\")",
    "print(f\" - Normal training sequences: {X_train_normal.shape[0]}\")",
    "print(f\" - This represents {X_train_normal.shape[0] / X_train.shape[0] * 100:.1f}% of training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save Preprocessed DataWe will save all preprocessed data for use in subsequent notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All preprocessed data saved complete",
      "",
      "üìÅ Saved files:",
      "- X_train.npy, X_train_normal.npy, X_val.npy, X_test.npy",
      "- y_train.npy, y_val.npy, y_test.npy",
      "- scaler.pkl, label_encoders.pkl, feature_names.pkl"
     ]
    }
   ],
   "source": [
    "# Create output directory",
    "os.makedirs('preprocessed_data', exist_ok=True)",
    "",
    "# Save data as numpy arrays",
    "np.save('preprocessed_data/X_train.npy', X_train)",
    "np.save('preprocessed_data/X_train_normal.npy', X_train_normal)",
    "np.save('preprocessed_data/X_val.npy', X_val)",
    "np.save('preprocessed_data/X_test.npy', X_test)",
    "np.save('preprocessed_data/y_train.npy', y_train)",
    "np.save('preprocessed_data/y_val.npy', y_val)",
    "np.save('preprocessed_data/y_test.npy', y_test)",
    "",
    "# Save preprocessing objects",
    "with open('preprocessed_data/scaler.pkl', 'wb') as f:",
    "pickle.dump(scaler, f)",
    "",
    "with open('preprocessed_data/label_encoders.pkl', 'wb') as f:",
    "pickle.dump(label_encoders, f)",
    "",
    "# Save feature names",
    "with open('preprocessed_data/feature_names.pkl', 'wb') as f:",
    "pickle.dump(list(features_df.columns), f)",
    "",
    "print(\" All preprocessed data saved complete\")",
    "print(\"\\n Saved files:\")",
    "print(\" - X_train.npy, X_train_normal.npy, X_val.npy, X_test.npy\")",
    "print(\" - y_train.npy, y_val.npy, y_test.npy\")",
    "print(\" - scaler.pkl, label_encoders.pkl, feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummaryIn this notebook, we successfully:1. Loaded the UNSW-NB15 dataset2. Inspected and cleaned the data3. Handled missing values4. Encoded categorical features5. Normalized all features to [0, 1] range6. Created sequences for temporal modeling7. Split data into train/validation/test sets8. Extracted normal traffic for autoencoder training9. Saved all preprocessed data---## Next StepsProceed to **Notebook 2: Visualization** to:- Explore feature distributions- Visualize attack patterns- Understand correlations- Gain insights into the dataset---**Note**: Make sure the `preprocessed_data/` directory contains all saved files before proceeding to the next notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}