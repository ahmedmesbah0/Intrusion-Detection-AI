{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 1: Data Preprocessing\n",
                "\n",
                "## Objective\n",
                "\n",
                "This notebook handles the preprocessing of the UNSW-NB15 dataset for our intrusion detection system. We will:\n",
                "\n",
                "1. Load the raw dataset from CSV files\n",
                "2. Perform exploratory data inspection\n",
                "3. Clean and handle missing values\n",
                "4. Normalize numerical features\n",
                "5. Encode categorical features\n",
                "6. Create sequences of network flows for temporal modeling\n",
                "7. Split data into train, validation, and test sets\n",
                "8. Save preprocessed data for subsequent notebooks\n",
                "\n",
                "---\n",
                "\n",
                "## Background: UNSW-NB15 Dataset\n",
                "\n",
                "The UNSW-NB15 dataset is a modern network intrusion detection dataset that contains:\n",
                "- **49 features** describing network flow characteristics\n",
                "- **Normal traffic** and **9 attack categories**\n",
                "- Real-world network traffic patterns\n",
                "\n",
                "**Attack Categories:**\n",
                "- Fuzzers: Attempts to discover vulnerabilities by sending random data\n",
                "- DoS: Denial of Service attacks\n",
                "- Exploits: Exploitation of known vulnerabilities\n",
                "- Reconnaissance: Scanning and probing\n",
                "- Shellcode: Code injection attacks\n",
                "- Analysis, Backdoor, Generic, Worms\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Required Libraries\n",
                "\n",
                "We'll use:\n",
                "- **pandas**: For data manipulation\n",
                "- **numpy**: For numerical operations\n",
                "- **sklearn**: For preprocessing and data splitting\n",
                "- **pickle**: For saving processed data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries imported successfully!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import os\n",
                "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load the Dataset\n",
                "\n",
                "The UNSW-NB15 dataset typically comes in two files:\n",
                "- Training set\n",
                "- Testing set\n",
                "\n",
                "We'll load both and combine them for our preprocessing pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading UNSW-NB15 dataset...\n",
                        "Dataset loaded successfully!\n",
                        "   - Training set: 82332 samples\n",
                        "   - Testing set: 175341 samples\n",
                        "   - Combined dataset: 257673 samples, 45 features\n"
                    ]
                }
            ],
            "source": [
                "# Define data paths\n",
                "TRAIN_PATH = 'data/UNSW_NB15_training-set.csv'\n",
                "TEST_PATH = 'data/UNSW_NB15_testing-set.csv'\n",
                "\n",
                "# Load the datasets\n",
                "print(\"Loading UNSW-NB15 dataset...\")\n",
                "train_df = pd.read_csv(TRAIN_PATH)\n",
                "test_df = pd.read_csv(TEST_PATH)\n",
                "\n",
                "# Combine both datasets for unified preprocessing\n",
                "df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
                "\n",
                "print(f\"Dataset loaded successfully!\")\n",
                "print(f\"   - Training set: {train_df.shape[0]} samples\")\n",
                "print(f\"   - Testing set: {test_df.shape[0]} samples\")\n",
                "print(f\"   - Combined dataset: {df.shape[0]} samples, {df.shape[1]} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Initial Data Inspection\n",
                "\n",
                "Let's examine the structure of our dataset to understand:\n",
                "- Feature names and types\n",
                "- Missing values\n",
                "- Basic statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First 5 rows of the dataset:\n",
                        "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
                        "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
                        "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
                        "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
                        "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
                        "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
                        "\n",
                        "          rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
                        "0   90909.0902  ...                 1               2             0   \n",
                        "1  125000.0003  ...                 1               2             0   \n",
                        "2  200000.0051  ...                 1               3             0   \n",
                        "3  166666.6608  ...                 1               3             0   \n",
                        "4  100000.0025  ...                 1               3             0   \n",
                        "\n",
                        "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
                        "0           0                 0           1           2                0   \n",
                        "1           0                 0           1           2                0   \n",
                        "2           0                 0           1           3                0   \n",
                        "3           0                 0           2           3                0   \n",
                        "4           0                 0           2           3                0   \n",
                        "\n",
                        "   attack_cat  label  \n",
                        "0      Normal      0  \n",
                        "1      Normal      0  \n",
                        "2      Normal      0  \n",
                        "3      Normal      0  \n",
                        "4      Normal      0  \n",
                        "\n",
                        "[5 rows x 45 columns]\n"
                    ]
                }
            ],
            "source": [
                "# Display first few rows\n",
                "print(\"First 5 rows of the dataset:\")\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ðŸ“‹ Dataset Information:\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 257673 entries, 0 to 257672\n",
                        "Data columns (total 45 columns):\n",
                        " #   Column             Non-Null Count   Dtype  \n",
                        "---  ------             --------------   -----  \n",
                        " 0   id                 257673 non-null  int64  \n",
                        " 1   dur                257673 non-null  float64\n",
                        " 2   proto              257673 non-null  object \n",
                        " 3   service            257673 non-null  object \n",
                        " 4   state              257673 non-null  object \n",
                        " 5   spkts              257673 non-null  int64  \n",
                        " 6   dpkts              257673 non-null  int64  \n",
                        " 7   sbytes             257673 non-null  int64  \n",
                        " 8   dbytes             257673 non-null  int64  \n",
                        " 9   rate               257673 non-null  float64\n",
                        " 10  sttl               257673 non-null  int64  \n",
                        " 11  dttl               257673 non-null  int64  \n",
                        " 12  sload              257673 non-null  float64\n",
                        " 13  dload              257673 non-null  float64\n",
                        " 14  sloss              257673 non-null  int64  \n",
                        " 15  dloss              257673 non-null  int64  \n",
                        " 16  sinpkt             257673 non-null  float64\n",
                        " 17  dinpkt             257673 non-null  float64\n",
                        " 18  sjit               257673 non-null  float64\n",
                        " 19  djit               257673 non-null  float64\n",
                        " 20  swin               257673 non-null  int64  \n",
                        " 21  stcpb              257673 non-null  int64  \n",
                        " 22  dtcpb              257673 non-null  int64  \n",
                        " 23  dwin               257673 non-null  int64  \n",
                        " 24  tcprtt             257673 non-null  float64\n",
                        " 25  synack             257673 non-null  float64\n",
                        " 26  ackdat             257673 non-null  float64\n",
                        " 27  smean              257673 non-null  int64  \n",
                        " 28  dmean              257673 non-null  int64  \n",
                        " 29  trans_depth        257673 non-null  int64  \n",
                        " 30  response_body_len  257673 non-null  int64  \n",
                        " 31  ct_srv_src         257673 non-null  int64  \n",
                        " 32  ct_state_ttl       257673 non-null  int64  \n",
                        " 33  ct_dst_ltm         257673 non-null  int64  \n",
                        " 34  ct_src_dport_ltm   257673 non-null  int64  \n",
                        " 35  ct_dst_sport_ltm   257673 non-null  int64  \n",
                        " 36  ct_dst_src_ltm     257673 non-null  int64  \n",
                        " 37  is_ftp_login       257673 non-null  int64  \n",
                        " 38  ct_ftp_cmd         257673 non-null  int64  \n",
                        " 39  ct_flw_http_mthd   257673 non-null  int64  \n",
                        " 40  ct_src_ltm         257673 non-null  int64  \n",
                        " 41  ct_srv_dst         257673 non-null  int64  \n",
                        " 42  is_sm_ips_ports    257673 non-null  int64  \n",
                        " 43  attack_cat         257673 non-null  object \n",
                        " 44  label              257673 non-null  int64  \n",
                        "dtypes: float64(11), int64(30), object(4)\n",
                        "memory usage: 88.5+ MB\n",
                        "None\n"
                    ]
                }
            ],
            "source": [
                "# Dataset information\n",
                "print(\"\\nðŸ“‹ Dataset Information:\")\n",
                "print(df.info())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Missing values per column:\n",
                        "No missing values found!\n"
                    ]
                }
            ],
            "source": [
                "# Check for missing values\n",
                "print(\"Missing values per column:\")\n",
                "missing = df.isnull().sum()\n",
                "missing = missing[missing > 0].sort_values(ascending=False)\n",
                "if len(missing) > 0:\n",
                "    print(missing)\n",
                "else:\n",
                "    print(\"No missing values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Class Distribution:\n",
                        "label\n",
                        "1    164673\n",
                        "0     93000\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Normal traffic: 93000 samples\n",
                        "Attack traffic: 164673 samples\n",
                        "Attack ratio: 63.91%\n",
                        "Attack Categories:\n",
                        "attack_cat\n",
                        "Normal            93000\n",
                        "Generic           58871\n",
                        "Exploits          44525\n",
                        "Fuzzers           24246\n",
                        "DoS               16353\n",
                        "Reconnaissance    13987\n",
                        "Analysis           2677\n",
                        "Backdoor           2329\n",
                        "Shellcode          1511\n",
                        "Worms               174\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Check class distribution\n",
                "print(\"Class Distribution:\")\n",
                "if 'label' in df.columns:\n",
                "    print(df['label'].value_counts())\n",
                "    print(f\"\\nNormal traffic: {(df['label'] == 0).sum()} samples\")\n",
                "    print(f\"Attack traffic: {(df['label'] == 1).sum()} samples\")\n",
                "    print(f\"Attack ratio: {(df['label'] == 1).sum() / len(df) * 100:.2f}%\")\n",
                "\n",
                "if 'attack_cat' in df.columns:\n",
                "    print(\"Attack Categories:\")\n",
                "    print(df['attack_cat'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Feature Selection and Cleaning\n",
                "\n",
                "We'll:\n",
                "1. Identify and separate feature types (numerical vs categorical)\n",
                "2. Remove irrelevant features (IDs, timestamps that don't add value)\n",
                "3. Handle missing values\n",
                "4. Store labels for later use"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Selected 42 features for modeling\n",
                        "\n",
                        "Feature columns: ['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n"
                    ]
                }
            ],
            "source": [
                "# Store labels separately\n",
                "labels = df['label'].values if 'label' in df.columns else None\n",
                "attack_categories = df['attack_cat'].values if 'attack_cat' in df.columns else None\n",
                "\n",
                "# Features to drop (non-predictive or identifier columns)\n",
                "# Adjust these based on your specific dataset columns\n",
                "columns_to_drop = ['id', 'label', 'attack_cat']\n",
                "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
                "\n",
                "# Create feature dataframe\n",
                "features_df = df.drop(columns=columns_to_drop, errors='ignore')\n",
                "\n",
                "print(f\"Selected {features_df.shape[1]} features for modeling\")\n",
                "print(f\"\\nFeature columns: {list(features_df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Numerical features: 39\n",
                        "Categorical features: 3\n",
                        "\n",
                        "Categorical columns: ['proto', 'service', 'state']\n"
                    ]
                }
            ],
            "source": [
                "# Identify categorical and numerical columns\n",
                "categorical_cols = features_df.select_dtypes(include=['object']).columns.tolist()\n",
                "numerical_cols = features_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "\n",
                "print(f\"Numerical features: {len(numerical_cols)}\")\n",
                "print(f\"Categorical features: {len(categorical_cols)}\")\n",
                "print(f\"\\nCategorical columns: {categorical_cols}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Handle Missing Values\n",
                "\n",
                "For missing values:\n",
                "- **Numerical features**: Fill with median\n",
                "- **Categorical features**: Fill with mode or 'unknown'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Missing values handled!\n"
                    ]
                }
            ],
            "source": [
                "# Handle missing values in numerical columns\n",
                "for col in numerical_cols:\n",
                "    if features_df[col].isnull().sum() > 0:\n",
                "        median_value = features_df[col].median()\n",
                "        features_df[col].fillna(median_value, inplace=True)\n",
                "        print(f\"Filled {col} with median: {median_value}\")\n",
                "\n",
                "# Handle missing values in categorical columns\n",
                "for col in categorical_cols:\n",
                "    if features_df[col].isnull().sum() > 0:\n",
                "        features_df[col].fillna('unknown', inplace=True)\n",
                "        print(f\"Filled {col} with 'unknown'\")\n",
                "\n",
                "print(\"\\nMissing values handled!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Encode Categorical Features\n",
                "\n",
                "We need to convert categorical variables (like protocol type, service, state) into numerical values using Label Encoding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Encoded proto: 133 unique values\n",
                        "Encoded service: 13 unique values\n",
                        "Encoded state: 11 unique values\n",
                        "\n",
                        "Categorical features encoded!\n"
                    ]
                }
            ],
            "source": [
                "# Initialize label encoders dictionary to save for later use\n",
                "label_encoders = {}\n",
                "\n",
                "# Encode categorical columns\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    features_df[col] = le.fit_transform(features_df[col].astype(str))\n",
                "    label_encoders[col] = le\n",
                "    print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
                "\n",
                "print(\"\\nCategorical features encoded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Normalize Numerical Features\n",
                "\n",
                "Neural networks perform better with normalized data. We'll use MinMaxScaler to scale all features to the range [0, 1]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Features normalized to range [0, 1]\n",
                        "\n",
                        "Scaled features shape: (257673, 42)\n",
                        "\n",
                        "Sample statistics after scaling:\n",
                        "                dur          proto        service          state  \\\n",
                        "count  2.576730e+05  257673.000000  257673.000000  257673.000000   \n",
                        "mean   2.077859e-02       0.834364       0.129659       0.434147   \n",
                        "std    9.957177e-02       0.161742       0.187162       0.088742   \n",
                        "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
                        "25%    1.333334e-07       0.856061       0.000000       0.400000   \n",
                        "50%    7.141668e-05       0.856061       0.000000       0.400000   \n",
                        "75%    1.142962e-02       0.901515       0.166667       0.500000   \n",
                        "max    1.000000e+00       1.000000       1.000000       1.000000   \n",
                        "\n",
                        "               spkts          dpkts         sbytes         dbytes  \\\n",
                        "count  257673.000000  257673.000000  257673.000000  257673.000000   \n",
                        "mean        0.001764       0.001680       0.000596       0.000982   \n",
                        "std         0.012771       0.010164       0.012105       0.009974   \n",
                        "min         0.000000       0.000000       0.000000       0.000000   \n",
                        "25%         0.000094       0.000000       0.000006       0.000000   \n",
                        "50%         0.000282       0.000182       0.000035       0.000012   \n",
                        "75%         0.001033       0.000908       0.000093       0.000073   \n",
                        "max         1.000000       1.000000       1.000000       1.000000   \n",
                        "\n",
                        "                rate           sttl  ...     ct_dst_ltm  ct_src_dport_ltm  \\\n",
                        "count  257673.000000  257673.000000  ...  257673.000000     257673.000000   \n",
                        "mean        0.091254       0.705886  ...       0.087077          0.073074   \n",
                        "std         0.160345       0.401915  ...       0.140927          0.140704   \n",
                        "min         0.000000       0.000000  ...       0.000000          0.000000   \n",
                        "25%         0.000031       0.243137  ...       0.000000          0.000000   \n",
                        "50%         0.002956       0.996078  ...       0.017241          0.000000   \n",
                        "75%         0.125000       0.996078  ...       0.086207          0.051724   \n",
                        "max         1.000000       1.000000  ...       1.000000          1.000000   \n",
                        "\n",
                        "       ct_dst_sport_ltm  ct_dst_src_ltm   is_ftp_login     ct_ftp_cmd  \\\n",
                        "count     257673.000000   257673.000000  257673.000000  257673.000000   \n",
                        "mean           0.067393        0.114421       0.003205       0.003212   \n",
                        "std            0.129589        0.173762       0.029023       0.029105   \n",
                        "min            0.000000        0.000000       0.000000       0.000000   \n",
                        "25%            0.000000        0.000000       0.000000       0.000000   \n",
                        "50%            0.000000        0.031250       0.000000       0.000000   \n",
                        "75%            0.044444        0.109375       0.000000       0.000000   \n",
                        "max            1.000000        1.000000       1.000000       1.000000   \n",
                        "\n",
                        "       ct_flw_http_mthd     ct_src_ltm     ct_srv_dst  is_sm_ips_ports  \n",
                        "count     257673.000000  257673.000000  257673.000000    257673.000000  \n",
                        "mean           0.004400       0.098306       0.133132         0.014274  \n",
                        "std            0.022728       0.142310       0.178275         0.118618  \n",
                        "min            0.000000       0.000000       0.000000         0.000000  \n",
                        "25%            0.000000       0.016949       0.016393         0.000000  \n",
                        "50%            0.000000       0.033898       0.049180         0.000000  \n",
                        "75%            0.000000       0.118644       0.163934         0.000000  \n",
                        "max            1.000000       1.000000       1.000000         1.000000  \n",
                        "\n",
                        "[8 rows x 42 columns]\n"
                    ]
                }
            ],
            "source": [
                "# Create and fit the scaler\n",
                "scaler = MinMaxScaler()\n",
                "features_scaled = scaler.fit_transform(features_df)\n",
                "\n",
                "# Convert back to DataFrame for easier handling\n",
                "features_scaled_df = pd.DataFrame(features_scaled, columns=features_df.columns)\n",
                "\n",
                "print(f\"Features normalized to range [0, 1]\")\n",
                "print(f\"\\nScaled features shape: {features_scaled_df.shape}\")\n",
                "print(f\"\\nSample statistics after scaling:\")\n",
                "print(features_scaled_df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Create Sequences for Temporal Modeling\n",
                "\n",
                "For our CNN+LSTM autoencoder, we need to create sequences of network flows. \n",
                "\n",
                "**Why sequences?**\n",
                "- Network attacks often span multiple packets/flows\n",
                "- Temporal patterns help identify anomalies\n",
                "- LSTM can learn time dependencies\n",
                "\n",
                "We'll use a sliding window approach to create fixed-length sequences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating sequences with length=10, stride=5...\n",
                        "\n",
                        "Sequences created!\n",
                        "   - Number of sequences: 51533\n",
                        "   - Sequence shape: (10, 42)\n",
                        "   - Normal sequences: 16981\n",
                        "   - Attack sequences: 34552\n"
                    ]
                }
            ],
            "source": [
                "def create_sequences(data, labels, sequence_length=10, stride=5):\n",
                "    \"\"\"\n",
                "    Create sequences from the dataset using a sliding window.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    data : array-like\n",
                "        Feature data (samples x features)\n",
                "    labels : array-like\n",
                "        Binary labels (0=normal, 1=attack)\n",
                "    sequence_length : int\n",
                "        Length of each sequence (number of flows)\n",
                "    stride : int\n",
                "        Step size for sliding window\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    sequences : ndarray\n",
                "        Sequences of shape (num_sequences, sequence_length, num_features)\n",
                "    sequence_labels : ndarray\n",
                "        Labels for each sequence (1 if any flow in sequence is attack)\n",
                "    \"\"\"\n",
                "    sequences = []\n",
                "    sequence_labels = []\n",
                "    \n",
                "    # Sliding window to create sequences\n",
                "    for i in range(0, len(data) - sequence_length + 1, stride):\n",
                "        # Extract sequence\n",
                "        seq = data[i:i + sequence_length]\n",
                "        \n",
                "        # Label is 1 if any flow in the sequence is an attack\n",
                "        label = labels[i:i + sequence_length]\n",
                "        seq_label = 1 if np.any(label == 1) else 0\n",
                "        \n",
                "        sequences.append(seq)\n",
                "        sequence_labels.append(seq_label)\n",
                "    \n",
                "    return np.array(sequences), np.array(sequence_labels)\n",
                "\n",
                "# Set sequence parameters\n",
                "SEQUENCE_LENGTH = 10  # Each sequence contains 10 network flows\n",
                "STRIDE = 5            # Move 5 flows forward for next sequence\n",
                "\n",
                "print(f\"Creating sequences with length={SEQUENCE_LENGTH}, stride={STRIDE}...\")\n",
                "\n",
                "# Create sequences\n",
                "X_sequences, y_sequences = create_sequences(\n",
                "    features_scaled_df.values, \n",
                "    labels, \n",
                "    sequence_length=SEQUENCE_LENGTH,\n",
                "    stride=STRIDE\n",
                ")\n",
                "\n",
                "print(f\"\\nSequences created!\")\n",
                "print(f\"   - Number of sequences: {X_sequences.shape[0]}\")\n",
                "print(f\"   - Sequence shape: {X_sequences.shape[1:]}\")\n",
                "print(f\"   - Normal sequences: {np.sum(y_sequences == 0)}\")\n",
                "print(f\"   - Attack sequences: {np.sum(y_sequences == 1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Split Data into Train, Validation, and Test Sets\n",
                "\n",
                "We'll split the data:\n",
                "- **70%** Training set (for model training)\n",
                "- **15%** Validation set (for hyperparameter tuning)\n",
                "- **15%** Test set (for final evaluation)\n",
                "\n",
                "**Important**: For autoencoder training, we'll primarily use normal traffic in the training set!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Data split completed!\n",
                        "\n",
                        "Training set:\n",
                        "   - Shape: (36073, 10, 42)\n",
                        "   - Normal: 11887, Attack: 24186\n",
                        "\n",
                        "Validation set:\n",
                        "   - Shape: (7730, 10, 42)\n",
                        "   - Normal: 2547, Attack: 5183\n",
                        "\n",
                        "Test set:\n",
                        "   - Shape: (7730, 10, 42)\n",
                        "   - Normal: 2547, Attack: 5183\n"
                    ]
                }
            ],
            "source": [
                "# First split: 70% train, 30% temp (for validation and test)\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(\n",
                "    X_sequences, y_sequences, \n",
                "    test_size=0.3, \n",
                "    random_state=42,\n",
                "    stratify=y_sequences  # Maintain class distribution\n",
                ")\n",
                "\n",
                "# Second split: Split temp into 50% validation, 50% test (15% each of total)\n",
                "X_val, X_test, y_val, y_test = train_test_split(\n",
                "    X_temp, y_temp, \n",
                "    test_size=0.5, \n",
                "    random_state=42,\n",
                "    stratify=y_temp\n",
                ")\n",
                "\n",
                "print(\" Data split completed!\")\n",
                "print(f\"\\nTraining set:\")\n",
                "print(f\"   - Shape: {X_train.shape}\")\n",
                "print(f\"   - Normal: {np.sum(y_train == 0)}, Attack: {np.sum(y_train == 1)}\")\n",
                "\n",
                "print(f\"\\nValidation set:\")\n",
                "print(f\"   - Shape: {X_val.shape}\")\n",
                "print(f\"   - Normal: {np.sum(y_val == 0)}, Attack: {np.sum(y_val == 1)}\")\n",
                "\n",
                "print(f\"\\nTest set:\")\n",
                "print(f\"   - Shape: {X_test.shape}\")\n",
                "print(f\"   - Normal: {np.sum(y_test == 0)}, Attack: {np.sum(y_test == 1)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Extract Normal Traffic for Autoencoder Training\n",
                "\n",
                "Autoencoders learn to reconstruct normal patterns. We train only on normal traffic, then use reconstruction error to detect anomalies (attacks)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracted normal traffic for autoencoder training\n",
                        "   - Normal training sequences: 11887\n",
                        "   - This represents 33.0% of training data\n"
                    ]
                }
            ],
            "source": [
                "# Extract only normal traffic for training the autoencoder\n",
                "X_train_normal = X_train[y_train == 0]\n",
                "\n",
                "print(f\"Extracted normal traffic for autoencoder training\")\n",
                "print(f\"   - Normal training sequences: {X_train_normal.shape[0]}\")\n",
                "print(f\"   - This represents {X_train_normal.shape[0] / X_train.shape[0] * 100:.1f}% of training data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Save Preprocessed Data\n",
                "\n",
                "We'll save all preprocessed data for use in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " All preprocessed data saved successfully!\n",
                        "\n",
                        " Saved files:\n",
                        "   - X_train.npy, X_train_normal.npy, X_val.npy, X_test.npy\n",
                        "   - y_train.npy, y_val.npy, y_test.npy\n",
                        "   - scaler.pkl, label_encoders.pkl, feature_names.pkl\n"
                    ]
                }
            ],
            "source": [
                "# Create output directory\n",
                "os.makedirs('preprocessed_data', exist_ok=True)\n",
                "\n",
                "# Save data as numpy arrays\n",
                "np.save('preprocessed_data/X_train.npy', X_train)\n",
                "np.save('preprocessed_data/X_train_normal.npy', X_train_normal)\n",
                "np.save('preprocessed_data/X_val.npy', X_val)\n",
                "np.save('preprocessed_data/X_test.npy', X_test)\n",
                "np.save('preprocessed_data/y_train.npy', y_train)\n",
                "np.save('preprocessed_data/y_val.npy', y_val)\n",
                "np.save('preprocessed_data/y_test.npy', y_test)\n",
                "\n",
                "# Save preprocessing objects\n",
                "with open('preprocessed_data/scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler, f)\n",
                "\n",
                "with open('preprocessed_data/label_encoders.pkl', 'wb') as f:\n",
                "    pickle.dump(label_encoders, f)\n",
                "\n",
                "# Save feature names\n",
                "with open('preprocessed_data/feature_names.pkl', 'wb') as f:\n",
                "    pickle.dump(list(features_df.columns), f)\n",
                "\n",
                "print(\" All preprocessed data saved successfully!\")\n",
                "print(\"\\n Saved files:\")\n",
                "print(\"   - X_train.npy, X_train_normal.npy, X_val.npy, X_test.npy\")\n",
                "print(\"   - y_train.npy, y_val.npy, y_test.npy\")\n",
                "print(\"   - scaler.pkl, label_encoders.pkl, feature_names.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this notebook, we successfully:\n",
                "\n",
                "1.  Loaded the UNSW-NB15 dataset\n",
                "2.  Inspected and cleaned the data\n",
                "3.  Handled missing values\n",
                "4.  Encoded categorical features\n",
                "5.  Normalized all features to [0, 1] range\n",
                "6.  Created sequences for temporal modeling\n",
                "7.  Split data into train/validation/test sets\n",
                "8.  Extracted normal traffic for autoencoder training\n",
                "9.  Saved all preprocessed data\n",
                "\n",
                "---\n",
                "\n",
                "##  Next Steps\n",
                "\n",
                "Proceed to **Notebook 2: Visualization** to:\n",
                "- Explore feature distributions\n",
                "- Visualize attack patterns\n",
                "- Understand correlations\n",
                "- Gain insights into the dataset\n",
                "\n",
                "---\n",
                "\n",
                "**Note**: Make sure the `preprocessed_data/` directory contains all saved files before proceeding to the next notebook!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
